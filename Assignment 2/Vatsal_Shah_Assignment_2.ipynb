{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Build a CNN for image recognition.\n",
    "\n",
    "## Due Date:  March 29, 11:59PM\n",
    "\n",
    "### Name: [Your-Name?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "1. In this assignment, you will build Convolutional Neural Network to classify CIFAR-10 Images.\n",
    "2. You can directly load dataset from many deep learning packages.\n",
    "3. You can use any deep learning packages such as pytorch, keras or tensorflow for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "\n",
    "1. You need to load cifar 10 data and split the entire training dataset into training and validation.\n",
    "2. You will implement a CNN model to classify cifar 10 images with provided structure.\n",
    "3. You need to plot the training and validation accuracy or loss obtained from above step.\n",
    "4. Then you can use tuned parameters to train using the entire training dataset.\n",
    "5. You should report the testing accuracy using the model with complete data.\n",
    "6. You may try to change the structure (e.g, add BN layer or dropout layer,...) and analyze your findings.\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Normalization is a technique to speed up training and help make the model more stable.\n",
    "- In simple words, batch normalization is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
    "\n",
    "- For more detailed information, you may refer to the original paper: https://arxiv.org/pdf/1502.03167.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: Values of $x$ over a mini-batch: $\\mathbf{B}$ = $\\{x_1,..., x_m\\};$\n",
    "- Output: $\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}$, $\\gamma, \\beta$ are learnable parameters\n",
    "\n",
    "Normalization of the Input:\n",
    "$$\\mu_{\\mathbf{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
    "$$\\sigma_{\\mathbf{B}}^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathbf{B}})^2$$\n",
    "$$\\hat{x_i} = \\frac{x_i - \\mu_{\\mathbf{B}}}{\\sqrt{\\sigma_{\\mathbf{B}}}^2 + \\epsilon}$$\n",
    "Re-scaling and Offsetting:\n",
    "$$y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma,\\beta}(x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of BN:\n",
    "1. Improves gradient flow through the network.\n",
    "2. Allows use of saturating nonlinearities and higher learning rates.\n",
    "3. Makes weights easier to initialize.\n",
    "4. Act as a form of regularization and may reduce the need for dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The batch normalization layer has already been implemented in many packages. You may simply call the function to build the layer. For example: torch.nn.BatchNorm2d() using pytroch package, keras.layers.BatchNormalization() using keras package.\n",
    "- The location of BN layer: Please make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in d:\\anaconda\\envs\\gpu\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\anaconda\\envs\\gpu\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=ec1c02fc5956d445fca7a9ec32915db1434b0ff0e880668796bc15bf540b9885\n",
      "  Stored in directory: c:\\users\\vapat\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Cifar-10 Data\n",
    "# This is just an example, you may load dataset from other packages.\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### If you can not load keras dataset, un-comment these two lines.\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))\n",
    "\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels (5 points)\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Implement a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    result = np.zeros((y.shape[0], 10))\n",
    "    for i, j in enumerate(y):\n",
    "      result[i][j] = 1\n",
    "    return result\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets (5 points)\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets: \n",
    "* a training set containing 40K samples: x_tr, y_tr\n",
    "* a validation set containing 10K samples: x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train_vec, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters (50 points)\n",
    "\n",
    "- Build a convolutional neural network model using the below structure:\n",
    "\n",
    "- It should have a structure of: Conv - ReLU - Max Pool - ConV - ReLU - Max Pool - Dense - ReLU - Dense - Softmax\n",
    "\n",
    "- In the graph 3@32x32 means the dimension of input image, 32@30x30 means it has 32 filters and the dimension now becomes 30x30 after the convolution.\n",
    "- All convolutional layers (Conv) should have stride = 1 and no padding.\n",
    "- Max Pooling has a pool size of 2 by 2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"network.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may use the validation data to tune the hyper-parameters (e.g., learning rate, and optimization algorithm)\n",
    "- Do NOT use test data for hyper-parameter tuning!!!\n",
    "- Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 626,378\n",
      "Trainable params: 626,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Conv2D(64, (4, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer and loss function\n",
    "import tensorflow\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Adadelta(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2500/2500 [==============================] - 26s 4ms/step - loss: 17.1324 - acc: 0.1221 - val_loss: 9.9859 - val_acc: 0.16491s - loss: 18.2478 - ETA: 0s - loss: - ETA: 0s - loss: 17.4177\n",
      "Epoch 2/200\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 8.5930 - acc: 0.1910 - val_loss: 7.5899 - val_acc: 0.2084\n",
      "Epoch 3/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 6.9625 - acc: 0.2197 - val_loss: 6.5077 - val_acc: 0.2294\n",
      "Epoch 4/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 6.0940 - acc: 0.2391 - val_loss: 5.8413 - val_acc: 0.2432\n",
      "Epoch 5/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 5.5302 - acc: 0.2528 - val_loss: 5.3827 - val_acc: 0.2539\n",
      "Epoch 6/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 5.1148 - acc: 0.2627 - val_loss: 5.0261 - val_acc: 0.2626\n",
      "Epoch 7/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 4.7875 - acc: 0.2727 - val_loss: 4.7354 - val_acc: 0.2718\n",
      "Epoch 8/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 4.5161 - acc: 0.2819 - val_loss: 4.4936 - val_acc: 0.2808\n",
      "Epoch 9/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 4.2861 - acc: 0.2915 - val_loss: 4.2830 - val_acc: 0.2855\n",
      "Epoch 10/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 4.0856 - acc: 0.2984 - val_loss: 4.1041 - val_acc: 0.2919\n",
      "Epoch 11/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.9090 - acc: 0.3051 - val_loss: 3.9412 - val_acc: 0.2973\n",
      "Epoch 12/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.7536 - acc: 0.3130 - val_loss: 3.7953 - val_acc: 0.3064\n",
      "Epoch 13/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.6125 - acc: 0.3184 - val_loss: 3.6599 - val_acc: 0.3096\n",
      "Epoch 14/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.4863 - acc: 0.3256 - val_loss: 3.5418 - val_acc: 0.3108\n",
      "Epoch 15/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 3.3729 - acc: 0.3294 - val_loss: 3.4319 - val_acc: 0.3190\n",
      "Epoch 16/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.2686 - acc: 0.3367 - val_loss: 3.3341 - val_acc: 0.3227\n",
      "Epoch 17/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 3.1729 - acc: 0.3410 - val_loss: 3.2552 - val_acc: 0.3255\n",
      "Epoch 18/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 3.0858 - acc: 0.3465 - val_loss: 3.1700 - val_acc: 0.3328\n",
      "Epoch 19/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 3.0060 - acc: 0.3494 - val_loss: 3.0827 - val_acc: 0.3363\n",
      "Epoch 20/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.9301 - acc: 0.3556 - val_loss: 3.0172 - val_acc: 0.3455\n",
      "Epoch 21/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.8604 - acc: 0.3602 - val_loss: 2.9512 - val_acc: 0.3495\n",
      "Epoch 22/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.7970 - acc: 0.3646 - val_loss: 2.8842 - val_acc: 0.3554\n",
      "Epoch 23/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.7341 - acc: 0.3697 - val_loss: 2.8383 - val_acc: 0.3562\n",
      "Epoch 24/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.6785 - acc: 0.3738 - val_loss: 2.7858 - val_acc: 0.3595\n",
      "Epoch 25/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.6251 - acc: 0.3778 - val_loss: 2.7261 - val_acc: 0.3666\n",
      "Epoch 26/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.5735 - acc: 0.3824 - val_loss: 2.6815 - val_acc: 0.3677\n",
      "Epoch 27/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.5264 - acc: 0.3858 - val_loss: 2.6319 - val_acc: 0.3749\n",
      "Epoch 28/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.4819 - acc: 0.3908 - val_loss: 2.5980 - val_acc: 0.3762\n",
      "Epoch 29/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.4377 - acc: 0.3935 - val_loss: 2.5514 - val_acc: 0.3813\n",
      "Epoch 30/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.3981 - acc: 0.3988 - val_loss: 2.5230 - val_acc: 0.3811\n",
      "Epoch 31/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.3598 - acc: 0.4021 - val_loss: 2.4861 - val_acc: 0.3873\n",
      "Epoch 32/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.3226 - acc: 0.4054 - val_loss: 2.4486 - val_acc: 0.3900\n",
      "Epoch 33/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.2878 - acc: 0.4090 - val_loss: 2.4221 - val_acc: 0.3946\n",
      "Epoch 34/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.2541 - acc: 0.4121 - val_loss: 2.3883 - val_acc: 0.3962\n",
      "Epoch 35/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.2224 - acc: 0.4155 - val_loss: 2.3562 - val_acc: 0.3983\n",
      "Epoch 36/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.1927 - acc: 0.4187 - val_loss: 2.3261 - val_acc: 0.4008\n",
      "Epoch 37/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.1622 - acc: 0.4218 - val_loss: 2.3037 - val_acc: 0.4030\n",
      "Epoch 38/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.1340 - acc: 0.4247 - val_loss: 2.2744 - val_acc: 0.4070\n",
      "Epoch 39/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.1067 - acc: 0.4270 - val_loss: 2.2544 - val_acc: 0.4123\n",
      "Epoch 40/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.0804 - acc: 0.4310 - val_loss: 2.2294 - val_acc: 0.4127\n",
      "Epoch 41/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.0550 - acc: 0.4327 - val_loss: 2.2035 - val_acc: 0.4139\n",
      "Epoch 42/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 2.0306 - acc: 0.4367 - val_loss: 2.1850 - val_acc: 0.4144\n",
      "Epoch 43/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 2.0077 - acc: 0.4391 - val_loss: 2.1708 - val_acc: 0.4151\n",
      "Epoch 44/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.9852 - acc: 0.4412 - val_loss: 2.1432 - val_acc: 0.4208\n",
      "Epoch 45/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.9634 - acc: 0.4449 - val_loss: 2.1228 - val_acc: 0.4222\n",
      "Epoch 46/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.9420 - acc: 0.4480 - val_loss: 2.1039 - val_acc: 0.4224\n",
      "Epoch 47/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.9211 - acc: 0.4513 - val_loss: 2.0837 - val_acc: 0.4269\n",
      "Epoch 48/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.9014 - acc: 0.4543 - val_loss: 2.0634 - val_acc: 0.4280\n",
      "Epoch 49/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.8821 - acc: 0.4566 - val_loss: 2.0494 - val_acc: 0.4329\n",
      "Epoch 50/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.8640 - acc: 0.4603 - val_loss: 2.0314 - val_acc: 0.4314\n",
      "Epoch 51/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.8455 - acc: 0.4616 - val_loss: 2.0148 - val_acc: 0.4350\n",
      "Epoch 52/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.8288 - acc: 0.4632 - val_loss: 1.9958 - val_acc: 0.4399\n",
      "Epoch 53/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.8125 - acc: 0.4662 - val_loss: 1.9807 - val_acc: 0.4394\n",
      "Epoch 54/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7950 - acc: 0.4711 - val_loss: 1.9707 - val_acc: 0.4409\n",
      "Epoch 55/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.7794 - acc: 0.4727 - val_loss: 1.9546 - val_acc: 0.4470\n",
      "Epoch 56/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7646 - acc: 0.4745 - val_loss: 1.9423 - val_acc: 0.4452\n",
      "Epoch 57/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7492 - acc: 0.4785 - val_loss: 1.9255 - val_acc: 0.4479\n",
      "Epoch 58/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7347 - acc: 0.4797 - val_loss: 1.9186 - val_acc: 0.4485\n",
      "Epoch 59/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7208 - acc: 0.4824 - val_loss: 1.8998 - val_acc: 0.4494\n",
      "Epoch 60/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.7070 - acc: 0.4845 - val_loss: 1.8859 - val_acc: 0.4545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.6937 - acc: 0.4871 - val_loss: 1.8796 - val_acc: 0.4527\n",
      "Epoch 62/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.6807 - acc: 0.4891 - val_loss: 1.8676 - val_acc: 0.4596\n",
      "Epoch 63/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.6680 - acc: 0.4909 - val_loss: 1.8503 - val_acc: 0.4572\n",
      "Epoch 64/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.6556 - acc: 0.4949 - val_loss: 1.8393 - val_acc: 0.4619\n",
      "Epoch 65/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.6434 - acc: 0.4967 - val_loss: 1.8278 - val_acc: 0.4600\n",
      "Epoch 66/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.6316 - acc: 0.4998 - val_loss: 1.8200 - val_acc: 0.4631\n",
      "Epoch 67/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.6201 - acc: 0.5004 - val_loss: 1.8099 - val_acc: 0.4669\n",
      "Epoch 68/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.6090 - acc: 0.5017 - val_loss: 1.8038 - val_acc: 0.4646\n",
      "Epoch 69/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5982 - acc: 0.5052 - val_loss: 1.7963 - val_acc: 0.4666\n",
      "Epoch 70/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5872 - acc: 0.5078 - val_loss: 1.7820 - val_acc: 0.4686\n",
      "Epoch 71/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5773 - acc: 0.5087 - val_loss: 1.7713 - val_acc: 0.4702\n",
      "Epoch 72/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.5671 - acc: 0.5102 - val_loss: 1.7648 - val_acc: 0.4700\n",
      "Epoch 73/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5567 - acc: 0.5146 - val_loss: 1.7550 - val_acc: 0.4738\n",
      "Epoch 74/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5477 - acc: 0.5155 - val_loss: 1.7428 - val_acc: 0.4739\n",
      "Epoch 75/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5381 - acc: 0.5166 - val_loss: 1.7378 - val_acc: 0.4764\n",
      "Epoch 76/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5287 - acc: 0.5191 - val_loss: 1.7293 - val_acc: 0.4800\n",
      "Epoch 77/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5193 - acc: 0.5218 - val_loss: 1.7233 - val_acc: 0.4786\n",
      "Epoch 78/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5108 - acc: 0.5232 - val_loss: 1.7141 - val_acc: 0.4784\n",
      "Epoch 79/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.5019 - acc: 0.5240 - val_loss: 1.7079 - val_acc: 0.4794\n",
      "Epoch 80/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4937 - acc: 0.5270 - val_loss: 1.7009 - val_acc: 0.4824\n",
      "Epoch 81/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4850 - acc: 0.5282 - val_loss: 1.6928 - val_acc: 0.4867\n",
      "Epoch 82/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.4760 - acc: 0.5306 - val_loss: 1.6907 - val_acc: 0.4840\n",
      "Epoch 83/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4689 - acc: 0.5332 - val_loss: 1.6831 - val_acc: 0.4862\n",
      "Epoch 84/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.4616 - acc: 0.5335 - val_loss: 1.6723 - val_acc: 0.4851\n",
      "Epoch 85/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4537 - acc: 0.5357 - val_loss: 1.6693 - val_acc: 0.4904\n",
      "Epoch 86/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4461 - acc: 0.5377 - val_loss: 1.6593 - val_acc: 0.4920\n",
      "Epoch 87/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4384 - acc: 0.5381 - val_loss: 1.6556 - val_acc: 0.4938\n",
      "Epoch 88/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4314 - acc: 0.5405 - val_loss: 1.6455 - val_acc: 0.4913\n",
      "Epoch 89/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4248 - acc: 0.5417 - val_loss: 1.6398 - val_acc: 0.4933\n",
      "Epoch 90/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4176 - acc: 0.5438 - val_loss: 1.6365 - val_acc: 0.4978\n",
      "Epoch 91/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4105 - acc: 0.5457 - val_loss: 1.6283 - val_acc: 0.4965\n",
      "Epoch 92/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.4037 - acc: 0.5469 - val_loss: 1.6260 - val_acc: 0.4961\n",
      "Epoch 93/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3973 - acc: 0.5477 - val_loss: 1.6186 - val_acc: 0.5001\n",
      "Epoch 94/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.3909 - acc: 0.5499 - val_loss: 1.6147 - val_acc: 0.5013\n",
      "Epoch 95/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3848 - acc: 0.5511 - val_loss: 1.6080 - val_acc: 0.5039\n",
      "Epoch 96/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3772 - acc: 0.5547 - val_loss: 1.6007 - val_acc: 0.5019\n",
      "Epoch 97/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3718 - acc: 0.5549 - val_loss: 1.5991 - val_acc: 0.5031\n",
      "Epoch 98/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.3656 - acc: 0.5569 - val_loss: 1.5916 - val_acc: 0.5014\n",
      "Epoch 99/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3595 - acc: 0.5582 - val_loss: 1.5919 - val_acc: 0.5023\n",
      "Epoch 100/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3533 - acc: 0.5590 - val_loss: 1.5899 - val_acc: 0.5033\n",
      "Epoch 101/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3473 - acc: 0.5606 - val_loss: 1.5799 - val_acc: 0.5085\n",
      "Epoch 102/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3419 - acc: 0.5634 - val_loss: 1.5749 - val_acc: 0.5067\n",
      "Epoch 103/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3362 - acc: 0.5647 - val_loss: 1.5686 - val_acc: 0.5087\n",
      "Epoch 104/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3306 - acc: 0.5666 - val_loss: 1.5668 - val_acc: 0.5097\n",
      "Epoch 105/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.3247 - acc: 0.5674 - val_loss: 1.5605 - val_acc: 0.5087\n",
      "Epoch 106/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.3197 - acc: 0.5687 - val_loss: 1.5540 - val_acc: 0.5118\n",
      "Epoch 107/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3146 - acc: 0.5708 - val_loss: 1.5513 - val_acc: 0.5124\n",
      "Epoch 108/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3081 - acc: 0.5721 - val_loss: 1.5509 - val_acc: 0.5114\n",
      "Epoch 109/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.3035 - acc: 0.5728 - val_loss: 1.5449 - val_acc: 0.5165\n",
      "Epoch 110/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2987 - acc: 0.5743 - val_loss: 1.5400 - val_acc: 0.5155\n",
      "Epoch 111/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2931 - acc: 0.5759 - val_loss: 1.5488 - val_acc: 0.5136\n",
      "Epoch 112/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.2884 - acc: 0.5765 - val_loss: 1.5382 - val_acc: 0.5199\n",
      "Epoch 113/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2832 - acc: 0.5776 - val_loss: 1.5319 - val_acc: 0.5158\n",
      "Epoch 114/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2789 - acc: 0.5795 - val_loss: 1.5222 - val_acc: 0.5189\n",
      "Epoch 115/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2736 - acc: 0.5806 - val_loss: 1.5197 - val_acc: 0.5203\n",
      "Epoch 116/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2691 - acc: 0.5826 - val_loss: 1.5184 - val_acc: 0.5209\n",
      "Epoch 117/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2643 - acc: 0.5831 - val_loss: 1.5128 - val_acc: 0.5185\n",
      "Epoch 118/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2592 - acc: 0.5855 - val_loss: 1.5110 - val_acc: 0.5194\n",
      "Epoch 119/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2548 - acc: 0.5865 - val_loss: 1.5070 - val_acc: 0.5222\n",
      "Epoch 120/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2503 - acc: 0.5881 - val_loss: 1.5030 - val_acc: 0.5226\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2460 - acc: 0.5890 - val_loss: 1.4981 - val_acc: 0.5247\n",
      "Epoch 122/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2415 - acc: 0.5896 - val_loss: 1.4939 - val_acc: 0.5246\n",
      "Epoch 123/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2366 - acc: 0.5917 - val_loss: 1.4920 - val_acc: 0.5236\n",
      "Epoch 124/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2331 - acc: 0.5925 - val_loss: 1.4883 - val_acc: 0.5257\n",
      "Epoch 125/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2283 - acc: 0.5932 - val_loss: 1.4854 - val_acc: 0.5249\n",
      "Epoch 126/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2239 - acc: 0.5950 - val_loss: 1.4820 - val_acc: 0.5250\n",
      "Epoch 127/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.2204 - acc: 0.5948 - val_loss: 1.4788 - val_acc: 0.5254\n",
      "Epoch 128/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2153 - acc: 0.5974 - val_loss: 1.4751 - val_acc: 0.5278\n",
      "Epoch 129/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2115 - acc: 0.5997 - val_loss: 1.4737 - val_acc: 0.5271\n",
      "Epoch 130/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2076 - acc: 0.6000 - val_loss: 1.4749 - val_acc: 0.5292\n",
      "Epoch 131/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.2032 - acc: 0.6009 - val_loss: 1.4733 - val_acc: 0.5301\n",
      "Epoch 132/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1995 - acc: 0.6021 - val_loss: 1.4658 - val_acc: 0.5311\n",
      "Epoch 133/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1956 - acc: 0.6029 - val_loss: 1.4630 - val_acc: 0.5310\n",
      "Epoch 134/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1913 - acc: 0.6046 - val_loss: 1.4572 - val_acc: 0.5305\n",
      "Epoch 135/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1876 - acc: 0.6058 - val_loss: 1.4593 - val_acc: 0.5307\n",
      "Epoch 136/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1842 - acc: 0.6073 - val_loss: 1.4525 - val_acc: 0.5332\n",
      "Epoch 137/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1807 - acc: 0.6070 - val_loss: 1.4544 - val_acc: 0.5335\n",
      "Epoch 138/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1765 - acc: 0.6084 - val_loss: 1.4526 - val_acc: 0.5354\n",
      "Epoch 139/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1731 - acc: 0.6093 - val_loss: 1.4474 - val_acc: 0.5350\n",
      "Epoch 140/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1690 - acc: 0.6110 - val_loss: 1.4462 - val_acc: 0.5337\n",
      "Epoch 141/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1661 - acc: 0.6126 - val_loss: 1.4412 - val_acc: 0.5362\n",
      "Epoch 142/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1623 - acc: 0.6126 - val_loss: 1.4374 - val_acc: 0.5374\n",
      "Epoch 143/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1585 - acc: 0.6136 - val_loss: 1.4348 - val_acc: 0.5371\n",
      "Epoch 144/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1549 - acc: 0.6155 - val_loss: 1.4321 - val_acc: 0.5360\n",
      "Epoch 145/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1513 - acc: 0.6170 - val_loss: 1.4288 - val_acc: 0.5387\n",
      "Epoch 146/200\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 1.1481 - acc: 0.6174 - val_loss: 1.4329 - val_acc: 0.5424\n",
      "Epoch 147/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1448 - acc: 0.6190 - val_loss: 1.4318 - val_acc: 0.5384\n",
      "Epoch 148/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1413 - acc: 0.6195 - val_loss: 1.4239 - val_acc: 0.5426\n",
      "Epoch 149/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1381 - acc: 0.6196 - val_loss: 1.4245 - val_acc: 0.5408\n",
      "Epoch 150/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1350 - acc: 0.6205 - val_loss: 1.4168 - val_acc: 0.5419\n",
      "Epoch 151/200\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 1.1312 - acc: 0.6224 - val_loss: 1.4176 - val_acc: 0.5430\n",
      "Epoch 152/200\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1281 - acc: 0.6248 - val_loss: 1.4156 - val_acc: 0.5433\n",
      "Epoch 153/200\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1246 - acc: 0.6238 - val_loss: 1.4110 - val_acc: 0.5439\n",
      "Epoch 154/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1216 - acc: 0.6248 - val_loss: 1.4105 - val_acc: 0.5424625 - ETA: 1s \n",
      "Epoch 155/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.1182 - acc: 0.6270 - val_loss: 1.4066 - val_acc: 0.5457\n",
      "Epoch 156/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1152 - acc: 0.6278 - val_loss: 1.4059 - val_acc: 0.5445\n",
      "Epoch 157/200\n",
      "2500/2500 [==============================] - 6s 3ms/step - loss: 1.1121 - acc: 0.6284 - val_loss: 1.4035 - val_acc: 0.5473\n",
      "Epoch 158/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1086 - acc: 0.6306 - val_loss: 1.4007 - val_acc: 0.5471\n",
      "Epoch 159/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1059 - acc: 0.6307 - val_loss: 1.3984 - val_acc: 0.5467\n",
      "Epoch 160/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.1029 - acc: 0.6324 - val_loss: 1.4000 - val_acc: 0.5466\n",
      "Epoch 161/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0997 - acc: 0.6341 - val_loss: 1.3980 - val_acc: 0.5478\n",
      "Epoch 162/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0964 - acc: 0.6326 - val_loss: 1.3956 - val_acc: 0.5472\n",
      "Epoch 163/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0931 - acc: 0.6336 - val_loss: 1.3890 - val_acc: 0.5465\n",
      "Epoch 164/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0906 - acc: 0.6363 - val_loss: 1.3867 - val_acc: 0.5504\n",
      "Epoch 165/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0881 - acc: 0.6353 - val_loss: 1.3893 - val_acc: 0.5494\n",
      "Epoch 166/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0852 - acc: 0.6371 - val_loss: 1.3871 - val_acc: 0.5493\n",
      "Epoch 167/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0821 - acc: 0.6389 - val_loss: 1.3827 - val_acc: 0.5486\n",
      "Epoch 168/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0791 - acc: 0.6394 - val_loss: 1.3835 - val_acc: 0.5504\n",
      "Epoch 169/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.0760 - acc: 0.6395 - val_loss: 1.3828 - val_acc: 0.5462\n",
      "Epoch 170/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.0736 - acc: 0.6407 - val_loss: 1.3817 - val_acc: 0.5487\n",
      "Epoch 171/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0707 - acc: 0.6425 - val_loss: 1.3758 - val_acc: 0.5498\n",
      "Epoch 172/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0681 - acc: 0.6423 - val_loss: 1.3754 - val_acc: 0.5537\n",
      "Epoch 173/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0654 - acc: 0.6429 - val_loss: 1.3708 - val_acc: 0.5533\n",
      "Epoch 174/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0622 - acc: 0.6449 - val_loss: 1.3754 - val_acc: 0.5536\n",
      "Epoch 175/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0599 - acc: 0.6452 - val_loss: 1.3669 - val_acc: 0.5545\n",
      "Epoch 176/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0575 - acc: 0.6461 - val_loss: 1.3709 - val_acc: 0.5521\n",
      "Epoch 177/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0546 - acc: 0.6474 - val_loss: 1.3658 - val_acc: 0.5543\n",
      "Epoch 178/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0521 - acc: 0.6471 - val_loss: 1.3614 - val_acc: 0.5552\n",
      "Epoch 179/200\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 1.0497 - acc: 0.6492 - val_loss: 1.3608 - val_acc: 0.5587\n",
      "Epoch 180/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0463 - acc: 0.6507 - val_loss: 1.3611 - val_acc: 0.5546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0441 - acc: 0.6493 - val_loss: 1.3587 - val_acc: 0.5537\n",
      "Epoch 182/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0414 - acc: 0.6520 - val_loss: 1.3568 - val_acc: 0.5580\n",
      "Epoch 183/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0388 - acc: 0.6514 - val_loss: 1.3591 - val_acc: 0.5567\n",
      "Epoch 184/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0366 - acc: 0.6512 - val_loss: 1.3525 - val_acc: 0.5586\n",
      "Epoch 185/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0338 - acc: 0.6539 - val_loss: 1.3575 - val_acc: 0.5563\n",
      "Epoch 186/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0315 - acc: 0.6539 - val_loss: 1.3498 - val_acc: 0.5571\n",
      "Epoch 187/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0291 - acc: 0.6564 - val_loss: 1.3489 - val_acc: 0.5587\n",
      "Epoch 188/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0272 - acc: 0.6548 - val_loss: 1.3461 - val_acc: 0.5598\n",
      "Epoch 189/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0240 - acc: 0.6566 - val_loss: 1.3483 - val_acc: 0.5572\n",
      "Epoch 190/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0216 - acc: 0.6574 - val_loss: 1.3442 - val_acc: 0.5609\n",
      "Epoch 191/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0195 - acc: 0.6589 - val_loss: 1.3429 - val_acc: 0.5614\n",
      "Epoch 192/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0165 - acc: 0.6594 - val_loss: 1.3413 - val_acc: 0.5621\n",
      "Epoch 193/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0144 - acc: 0.6607 - val_loss: 1.3416 - val_acc: 0.5636\n",
      "Epoch 194/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0125 - acc: 0.6612 - val_loss: 1.3380 - val_acc: 0.5605\n",
      "Epoch 195/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0096 - acc: 0.6611 - val_loss: 1.3387 - val_acc: 0.5636\n",
      "Epoch 196/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0069 - acc: 0.6643 - val_loss: 1.3405 - val_acc: 0.5624\n",
      "Epoch 197/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0048 - acc: 0.6632 - val_loss: 1.3376 - val_acc: 0.5659\n",
      "Epoch 198/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0028 - acc: 0.6638 - val_loss: 1.3337 - val_acc: 0.5603\n",
      "Epoch 199/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 1.0001 - acc: 0.6644 - val_loss: 1.3349 - val_acc: 0.5641\n",
      "Epoch 200/200\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 0.9985 - acc: 0.6657 - val_loss: 1.3350 - val_acc: 0.5662\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store model parameters/loss values\n",
    "\n",
    "model1 = model.fit(x_tr, y_tr, batch_size=16, epochs=200, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot the training and validation loss curve versus epochs. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(range(len(model1.history[\"loss\"])), model1.history[\"loss\"], \"bo\", label=\"Training loss\")\n",
    "plt.plot(range(len(model1.history[\"val_loss\"])), model1.history[\"val_loss\"], \"r\", label=\"Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train (again) and evaluate the model (5 points)\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Adadelta(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0635 - acc: 0.6462 - val_loss: 1.3205 - val_acc: 0.5653\n",
      "Epoch 2/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0593 - acc: 0.6462 - val_loss: 1.3117 - val_acc: 0.5696\n",
      "Epoch 3/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0554 - acc: 0.6492 - val_loss: 1.3062 - val_acc: 0.5696\n",
      "Epoch 4/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0525 - acc: 0.6497 - val_loss: 1.2948 - val_acc: 0.5731\n",
      "Epoch 5/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0495 - acc: 0.6499 - val_loss: 1.2902 - val_acc: 0.5750\n",
      "Epoch 6/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0465 - acc: 0.6527 - val_loss: 1.2806 - val_acc: 0.5764\n",
      "Epoch 7/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0430 - acc: 0.6529 - val_loss: 1.2786 - val_acc: 0.5740\n",
      "Epoch 8/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0401 - acc: 0.6519 - val_loss: 1.2712 - val_acc: 0.5773\n",
      "Epoch 9/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0375 - acc: 0.6527 - val_loss: 1.2657 - val_acc: 0.5791\n",
      "Epoch 10/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0344 - acc: 0.6545 - val_loss: 1.2593 - val_acc: 0.5801\n",
      "Epoch 11/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0313 - acc: 0.6562 - val_loss: 1.2549 - val_acc: 0.5817\n",
      "Epoch 12/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0285 - acc: 0.6567 - val_loss: 1.2509 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0260 - acc: 0.6573 - val_loss: 1.2475 - val_acc: 0.5837\n",
      "Epoch 14/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0234 - acc: 0.6572 - val_loss: 1.2392 - val_acc: 0.5871\n",
      "Epoch 15/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0207 - acc: 0.6586 - val_loss: 1.2349 - val_acc: 0.5875\n",
      "Epoch 16/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.0174 - acc: 0.6595 - val_loss: 1.2340 - val_acc: 0.5910\n",
      "Epoch 17/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0152 - acc: 0.6599 - val_loss: 1.2239 - val_acc: 0.5907\n",
      "Epoch 18/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0125 - acc: 0.6604 - val_loss: 1.2222 - val_acc: 0.5939\n",
      "Epoch 19/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0095 - acc: 0.6620 - val_loss: 1.2173 - val_acc: 0.5918\n",
      "Epoch 20/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0073 - acc: 0.6631 - val_loss: 1.2119 - val_acc: 0.5944\n",
      "Epoch 21/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0048 - acc: 0.6631 - val_loss: 1.2056 - val_acc: 0.5968\n",
      "Epoch 22/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 1.0020 - acc: 0.6635 - val_loss: 1.2008 - val_acc: 0.5967\n",
      "Epoch 23/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9993 - acc: 0.6652 - val_loss: 1.1971 - val_acc: 0.5986\n",
      "Epoch 24/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9972 - acc: 0.6668 - val_loss: 1.1993 - val_acc: 0.5951\n",
      "Epoch 25/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9945 - acc: 0.6657 - val_loss: 1.1906 - val_acc: 0.6020\n",
      "Epoch 26/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9922 - acc: 0.6671 - val_loss: 1.1914 - val_acc: 0.6012\n",
      "Epoch 27/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9896 - acc: 0.6691 - val_loss: 1.1817 - val_acc: 0.6011\n",
      "Epoch 28/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9871 - acc: 0.6683 - val_loss: 1.1771 - val_acc: 0.6055\n",
      "Epoch 29/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9848 - acc: 0.6699 - val_loss: 1.1719 - val_acc: 0.6064\n",
      "Epoch 30/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9825 - acc: 0.6708 - val_loss: 1.1697 - val_acc: 0.6075\n",
      "Epoch 31/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9804 - acc: 0.6711 - val_loss: 1.1647 - val_acc: 0.6091\n",
      "Epoch 32/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9782 - acc: 0.6721 - val_loss: 1.1640 - val_acc: 0.6120\n",
      "Epoch 33/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9756 - acc: 0.6723 - val_loss: 1.1583 - val_acc: 0.6072\n",
      "Epoch 34/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9734 - acc: 0.6732 - val_loss: 1.1533 - val_acc: 0.6116\n",
      "Epoch 35/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9706 - acc: 0.6734 - val_loss: 1.1513 - val_acc: 0.6077\n",
      "Epoch 36/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9685 - acc: 0.6744 - val_loss: 1.1462 - val_acc: 0.6150\n",
      "Epoch 37/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9663 - acc: 0.6753 - val_loss: 1.1435 - val_acc: 0.6141\n",
      "Epoch 38/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9642 - acc: 0.6759 - val_loss: 1.1389 - val_acc: 0.6145\n",
      "Epoch 39/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9616 - acc: 0.6766 - val_loss: 1.1392 - val_acc: 0.6191\n",
      "Epoch 40/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9602 - acc: 0.6780 - val_loss: 1.1353 - val_acc: 0.6167\n",
      "Epoch 41/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9579 - acc: 0.6784 - val_loss: 1.1269 - val_acc: 0.6170\n",
      "Epoch 42/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9552 - acc: 0.6799 - val_loss: 1.1295 - val_acc: 0.6177\n",
      "Epoch 43/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9534 - acc: 0.6802 - val_loss: 1.1241 - val_acc: 0.6197\n",
      "Epoch 44/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9512 - acc: 0.6802 - val_loss: 1.1221 - val_acc: 0.6220\n",
      "Epoch 45/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9494 - acc: 0.6820 - val_loss: 1.1167 - val_acc: 0.6227\n",
      "Epoch 46/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9475 - acc: 0.6817 - val_loss: 1.1114 - val_acc: 0.6236\n",
      "Epoch 47/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9450 - acc: 0.6822 - val_loss: 1.1081 - val_acc: 0.6252\n",
      "Epoch 48/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9434 - acc: 0.6833 - val_loss: 1.1039 - val_acc: 0.6273\n",
      "Epoch 49/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9411 - acc: 0.6842 - val_loss: 1.1037 - val_acc: 0.6279\n",
      "Epoch 50/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9389 - acc: 0.6852 - val_loss: 1.0997 - val_acc: 0.6285\n",
      "Epoch 51/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9371 - acc: 0.6855 - val_loss: 1.0924 - val_acc: 0.6292\n",
      "Epoch 52/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9348 - acc: 0.6857 - val_loss: 1.0949 - val_acc: 0.6295\n",
      "Epoch 53/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9329 - acc: 0.6870 - val_loss: 1.0921 - val_acc: 0.6301\n",
      "Epoch 54/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9305 - acc: 0.6872 - val_loss: 1.0846 - val_acc: 0.6342\n",
      "Epoch 55/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9291 - acc: 0.6875 - val_loss: 1.0825 - val_acc: 0.6338\n",
      "Epoch 56/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9268 - acc: 0.6878 - val_loss: 1.0784 - val_acc: 0.6314\n",
      "Epoch 57/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9249 - acc: 0.6895 - val_loss: 1.0775 - val_acc: 0.6353\n",
      "Epoch 58/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9231 - acc: 0.6908 - val_loss: 1.0738 - val_acc: 0.6379\n",
      "Epoch 59/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9216 - acc: 0.6909 - val_loss: 1.0690 - val_acc: 0.6384\n",
      "Epoch 60/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9190 - acc: 0.6911 - val_loss: 1.0664 - val_acc: 0.6393\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9168 - acc: 0.6921 - val_loss: 1.0649 - val_acc: 0.6406\n",
      "Epoch 62/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9153 - acc: 0.6922 - val_loss: 1.0612 - val_acc: 0.6402\n",
      "Epoch 63/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9134 - acc: 0.6931 - val_loss: 1.0563 - val_acc: 0.6446\n",
      "Epoch 64/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9117 - acc: 0.6938 - val_loss: 1.0542 - val_acc: 0.6427\n",
      "Epoch 65/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9100 - acc: 0.6938 - val_loss: 1.0545 - val_acc: 0.6415\n",
      "Epoch 66/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9080 - acc: 0.6945 - val_loss: 1.0527 - val_acc: 0.6419\n",
      "Epoch 67/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9064 - acc: 0.6959 - val_loss: 1.0483 - val_acc: 0.6460\n",
      "Epoch 68/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9043 - acc: 0.6969 - val_loss: 1.0453 - val_acc: 0.6469\n",
      "Epoch 69/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.9022 - acc: 0.6970 - val_loss: 1.0413 - val_acc: 0.6485\n",
      "Epoch 70/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.9009 - acc: 0.6983 - val_loss: 1.0408 - val_acc: 0.6489\n",
      "Epoch 71/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8987 - acc: 0.6985 - val_loss: 1.0425 - val_acc: 0.6485\n",
      "Epoch 72/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8972 - acc: 0.6978 - val_loss: 1.0337 - val_acc: 0.6516\n",
      "Epoch 73/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8954 - acc: 0.6999 - val_loss: 1.0294 - val_acc: 0.6506\n",
      "Epoch 74/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8934 - acc: 0.6999 - val_loss: 1.0300 - val_acc: 0.6525\n",
      "Epoch 75/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8920 - acc: 0.7004 - val_loss: 1.0273 - val_acc: 0.6508\n",
      "Epoch 76/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8900 - acc: 0.7014 - val_loss: 1.0233 - val_acc: 0.6549\n",
      "Epoch 77/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8884 - acc: 0.7015 - val_loss: 1.0214 - val_acc: 0.6546\n",
      "Epoch 78/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8867 - acc: 0.7013 - val_loss: 1.0169 - val_acc: 0.6562\n",
      "Epoch 79/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8847 - acc: 0.7035 - val_loss: 1.0158 - val_acc: 0.6547\n",
      "Epoch 80/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8824 - acc: 0.7033 - val_loss: 1.0132 - val_acc: 0.6583\n",
      "Epoch 81/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8812 - acc: 0.7039 - val_loss: 1.0113 - val_acc: 0.6569\n",
      "Epoch 82/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8795 - acc: 0.7045 - val_loss: 1.0053 - val_acc: 0.6596\n",
      "Epoch 83/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8780 - acc: 0.7046 - val_loss: 1.0109 - val_acc: 0.6552\n",
      "Epoch 84/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8763 - acc: 0.7053 - val_loss: 1.0058 - val_acc: 0.6572\n",
      "Epoch 85/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8744 - acc: 0.7066 - val_loss: 1.0067 - val_acc: 0.6578\n",
      "Epoch 86/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8730 - acc: 0.7058 - val_loss: 1.0016 - val_acc: 0.6595\n",
      "Epoch 87/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8715 - acc: 0.7068 - val_loss: 0.9938 - val_acc: 0.6634\n",
      "Epoch 88/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8694 - acc: 0.7091 - val_loss: 0.9933 - val_acc: 0.6604\n",
      "Epoch 89/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8681 - acc: 0.7084 - val_loss: 0.9882 - val_acc: 0.6651\n",
      "Epoch 90/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8661 - acc: 0.7095 - val_loss: 0.9878 - val_acc: 0.6653\n",
      "Epoch 91/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8647 - acc: 0.7090 - val_loss: 0.9862 - val_acc: 0.6658\n",
      "Epoch 92/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8631 - acc: 0.7105 - val_loss: 0.9858 - val_acc: 0.6675\n",
      "Epoch 93/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8616 - acc: 0.7114 - val_loss: 0.9785 - val_acc: 0.6687\n",
      "Epoch 94/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8601 - acc: 0.7115 - val_loss: 0.9790 - val_acc: 0.6679\n",
      "Epoch 95/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8584 - acc: 0.7112 - val_loss: 0.9789 - val_acc: 0.6687\n",
      "Epoch 96/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8566 - acc: 0.7126 - val_loss: 0.9731 - val_acc: 0.6709\n",
      "Epoch 97/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8552 - acc: 0.7120 - val_loss: 0.9743 - val_acc: 0.6700\n",
      "Epoch 98/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8536 - acc: 0.7139 - val_loss: 0.9691 - val_acc: 0.6704\n",
      "Epoch 99/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8520 - acc: 0.7127 - val_loss: 0.9667 - val_acc: 0.6725\n",
      "Epoch 100/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8509 - acc: 0.7149 - val_loss: 0.9658 - val_acc: 0.6742\n",
      "Epoch 101/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8489 - acc: 0.7145 - val_loss: 0.9633 - val_acc: 0.6729\n",
      "Epoch 102/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8473 - acc: 0.7171 - val_loss: 0.9599 - val_acc: 0.6736\n",
      "Epoch 103/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8463 - acc: 0.7168 - val_loss: 0.9583 - val_acc: 0.6760\n",
      "Epoch 104/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8441 - acc: 0.7158 - val_loss: 0.9594 - val_acc: 0.6737\n",
      "Epoch 105/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8430 - acc: 0.7164 - val_loss: 0.9547 - val_acc: 0.6782\n",
      "Epoch 106/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8417 - acc: 0.7171 - val_loss: 0.9520 - val_acc: 0.6788\n",
      "Epoch 107/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8400 - acc: 0.7183 - val_loss: 0.9534 - val_acc: 0.6768\n",
      "Epoch 108/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8385 - acc: 0.7199 - val_loss: 0.9524 - val_acc: 0.6750\n",
      "Epoch 109/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8371 - acc: 0.7200 - val_loss: 0.9449 - val_acc: 0.6786\n",
      "Epoch 110/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8357 - acc: 0.7194 - val_loss: 0.9425 - val_acc: 0.6800\n",
      "Epoch 111/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8339 - acc: 0.7199 - val_loss: 0.9413 - val_acc: 0.6809\n",
      "Epoch 112/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8326 - acc: 0.7214 - val_loss: 0.9378 - val_acc: 0.6827\n",
      "Epoch 113/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8313 - acc: 0.7211 - val_loss: 0.9363 - val_acc: 0.6812\n",
      "Epoch 114/200\n",
      "3125/3125 [==============================] - 10s 3ms/step - loss: 0.8294 - acc: 0.7225 - val_loss: 0.9378 - val_acc: 0.6808\n",
      "Epoch 115/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8283 - acc: 0.7231 - val_loss: 0.9354 - val_acc: 0.6826\n",
      "Epoch 116/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8269 - acc: 0.7239 - val_loss: 0.9385 - val_acc: 0.6805\n",
      "Epoch 117/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8256 - acc: 0.7231 - val_loss: 0.9355 - val_acc: 0.6828\n",
      "Epoch 118/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8239 - acc: 0.7248 - val_loss: 0.9288 - val_acc: 0.6852\n",
      "Epoch 119/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8231 - acc: 0.7236 - val_loss: 0.9265 - val_acc: 0.6846\n",
      "Epoch 120/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8212 - acc: 0.7244 - val_loss: 0.9245 - val_acc: 0.6841\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8204 - acc: 0.7246 - val_loss: 0.9214 - val_acc: 0.6867\n",
      "Epoch 122/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8184 - acc: 0.7259 - val_loss: 0.9284 - val_acc: 0.6827\n",
      "Epoch 123/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8171 - acc: 0.7258 - val_loss: 0.9167 - val_acc: 0.6893\n",
      "Epoch 124/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8156 - acc: 0.7274 - val_loss: 0.9199 - val_acc: 0.6862\n",
      "Epoch 125/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8144 - acc: 0.7262 - val_loss: 0.9171 - val_acc: 0.6889\n",
      "Epoch 126/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8129 - acc: 0.7274 - val_loss: 0.9112 - val_acc: 0.6905\n",
      "Epoch 127/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8120 - acc: 0.7280 - val_loss: 0.9108 - val_acc: 0.6903\n",
      "Epoch 128/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.8103 - acc: 0.7288 - val_loss: 0.9063 - val_acc: 0.6933\n",
      "Epoch 129/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8088 - acc: 0.7289 - val_loss: 0.9085 - val_acc: 0.6902\n",
      "Epoch 130/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8075 - acc: 0.7293 - val_loss: 0.9039 - val_acc: 0.6930\n",
      "Epoch 131/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8063 - acc: 0.7306 - val_loss: 0.9018 - val_acc: 0.6937\n",
      "Epoch 132/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8049 - acc: 0.7305 - val_loss: 0.9011 - val_acc: 0.6926\n",
      "Epoch 133/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8035 - acc: 0.7307 - val_loss: 0.8998 - val_acc: 0.6929\n",
      "Epoch 134/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8021 - acc: 0.7312 - val_loss: 0.8971 - val_acc: 0.6967\n",
      "Epoch 135/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.8007 - acc: 0.7313 - val_loss: 0.8964 - val_acc: 0.6954\n",
      "Epoch 136/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7996 - acc: 0.7317 - val_loss: 0.8947 - val_acc: 0.6979\n",
      "Epoch 137/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7982 - acc: 0.7332 - val_loss: 0.8916 - val_acc: 0.6965\n",
      "Epoch 138/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7971 - acc: 0.7326 - val_loss: 0.8954 - val_acc: 0.6985\n",
      "Epoch 139/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7956 - acc: 0.7341 - val_loss: 0.8879 - val_acc: 0.6979\n",
      "Epoch 140/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7945 - acc: 0.7337 - val_loss: 0.8900 - val_acc: 0.6963\n",
      "Epoch 141/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7933 - acc: 0.7342 - val_loss: 0.8836 - val_acc: 0.6998\n",
      "Epoch 142/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7919 - acc: 0.7340 - val_loss: 0.8855 - val_acc: 0.7009\n",
      "Epoch 143/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7909 - acc: 0.7352 - val_loss: 0.8828 - val_acc: 0.7015\n",
      "Epoch 144/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7890 - acc: 0.7363 - val_loss: 0.8788 - val_acc: 0.7025\n",
      "Epoch 145/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7882 - acc: 0.7363 - val_loss: 0.8766 - val_acc: 0.7018\n",
      "Epoch 146/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7870 - acc: 0.7373 - val_loss: 0.8771 - val_acc: 0.7027\n",
      "Epoch 147/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7855 - acc: 0.7379 - val_loss: 0.8750 - val_acc: 0.7051\n",
      "Epoch 148/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7844 - acc: 0.7368 - val_loss: 0.8747 - val_acc: 0.7052\n",
      "Epoch 149/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7829 - acc: 0.7380 - val_loss: 0.8708 - val_acc: 0.7051\n",
      "Epoch 150/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7817 - acc: 0.7394 - val_loss: 0.8699 - val_acc: 0.7062\n",
      "Epoch 151/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7807 - acc: 0.7394 - val_loss: 0.8686 - val_acc: 0.7050\n",
      "Epoch 152/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7793 - acc: 0.7390 - val_loss: 0.8690 - val_acc: 0.7051\n",
      "Epoch 153/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7781 - acc: 0.7394 - val_loss: 0.8650 - val_acc: 0.7081\n",
      "Epoch 154/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7764 - acc: 0.7399 - val_loss: 0.8627 - val_acc: 0.7065\n",
      "Epoch 155/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7756 - acc: 0.7402 - val_loss: 0.8637 - val_acc: 0.7078\n",
      "Epoch 156/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7746 - acc: 0.7415 - val_loss: 0.8600 - val_acc: 0.7089\n",
      "Epoch 157/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7730 - acc: 0.7401 - val_loss: 0.8594 - val_acc: 0.7077\n",
      "Epoch 158/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7723 - acc: 0.7437 - val_loss: 0.8579 - val_acc: 0.7098\n",
      "Epoch 159/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7710 - acc: 0.7421 - val_loss: 0.8582 - val_acc: 0.7079\n",
      "Epoch 160/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7698 - acc: 0.7427 - val_loss: 0.8524 - val_acc: 0.7110\n",
      "Epoch 161/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7681 - acc: 0.7436 - val_loss: 0.8561 - val_acc: 0.7091\n",
      "Epoch 162/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7674 - acc: 0.7433 - val_loss: 0.8568 - val_acc: 0.7093\n",
      "Epoch 163/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7660 - acc: 0.7447 - val_loss: 0.8473 - val_acc: 0.7138\n",
      "Epoch 164/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7643 - acc: 0.7457 - val_loss: 0.8480 - val_acc: 0.7124\n",
      "Epoch 165/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7636 - acc: 0.7445 - val_loss: 0.8444 - val_acc: 0.7152\n",
      "Epoch 166/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7630 - acc: 0.7464 - val_loss: 0.8468 - val_acc: 0.7133\n",
      "Epoch 167/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7612 - acc: 0.7454 - val_loss: 0.8446 - val_acc: 0.7137\n",
      "Epoch 168/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7604 - acc: 0.7463 - val_loss: 0.8433 - val_acc: 0.7157\n",
      "Epoch 169/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7591 - acc: 0.7472 - val_loss: 0.8405 - val_acc: 0.7145\n",
      "Epoch 170/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7581 - acc: 0.7463 - val_loss: 0.8413 - val_acc: 0.7155\n",
      "Epoch 171/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7570 - acc: 0.7467 - val_loss: 0.8380 - val_acc: 0.7150\n",
      "Epoch 172/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7557 - acc: 0.7478 - val_loss: 0.8360 - val_acc: 0.7159\n",
      "Epoch 173/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7544 - acc: 0.7492 - val_loss: 0.8315 - val_acc: 0.7178\n",
      "Epoch 174/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7534 - acc: 0.7484 - val_loss: 0.8308 - val_acc: 0.7189\n",
      "Epoch 175/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7520 - acc: 0.7493 - val_loss: 0.8304 - val_acc: 0.7195\n",
      "Epoch 176/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7512 - acc: 0.7502 - val_loss: 0.8302 - val_acc: 0.7206\n",
      "Epoch 177/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7499 - acc: 0.7504 - val_loss: 0.8280 - val_acc: 0.7190\n",
      "Epoch 178/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7488 - acc: 0.7500 - val_loss: 0.8277 - val_acc: 0.7190\n",
      "Epoch 179/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7480 - acc: 0.7513 - val_loss: 0.8257 - val_acc: 0.7219\n",
      "Epoch 180/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7464 - acc: 0.7512 - val_loss: 0.8232 - val_acc: 0.7207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7454 - acc: 0.7514 - val_loss: 0.8222 - val_acc: 0.7225\n",
      "Epoch 182/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7447 - acc: 0.7523 - val_loss: 0.8209 - val_acc: 0.7225\n",
      "Epoch 183/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7434 - acc: 0.7521 - val_loss: 0.8237 - val_acc: 0.7216\n",
      "Epoch 184/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7420 - acc: 0.7520 - val_loss: 0.8228 - val_acc: 0.7220\n",
      "Epoch 185/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7408 - acc: 0.7537 - val_loss: 0.8161 - val_acc: 0.7248\n",
      "Epoch 186/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7396 - acc: 0.7533 - val_loss: 0.8132 - val_acc: 0.7270\n",
      "Epoch 187/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7390 - acc: 0.7542 - val_loss: 0.8171 - val_acc: 0.7218\n",
      "Epoch 188/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7379 - acc: 0.7543 - val_loss: 0.8140 - val_acc: 0.7238\n",
      "Epoch 189/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7367 - acc: 0.7542 - val_loss: 0.8131 - val_acc: 0.7282\n",
      "Epoch 190/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7354 - acc: 0.7554 - val_loss: 0.8091 - val_acc: 0.7270\n",
      "Epoch 191/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7347 - acc: 0.7564 - val_loss: 0.8078 - val_acc: 0.7277\n",
      "Epoch 192/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7338 - acc: 0.7561 - val_loss: 0.8057 - val_acc: 0.7282\n",
      "Epoch 193/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7324 - acc: 0.7557 - val_loss: 0.8034 - val_acc: 0.7295\n",
      "Epoch 194/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7313 - acc: 0.7570 - val_loss: 0.8013 - val_acc: 0.7306\n",
      "Epoch 195/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7301 - acc: 0.7570 - val_loss: 0.8043 - val_acc: 0.7289\n",
      "Epoch 196/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7294 - acc: 0.7577 - val_loss: 0.7990 - val_acc: 0.7328\n",
      "Epoch 197/200\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.7285 - acc: 0.7576 - val_loss: 0.7977 - val_acc: 0.7321\n",
      "Epoch 198/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7272 - acc: 0.7578 - val_loss: 0.8054 - val_acc: 0.7264\n",
      "Epoch 199/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7263 - acc: 0.7579 - val_loss: 0.7969 - val_acc: 0.7303\n",
      "Epoch 200/200\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.7250 - acc: 0.7595 - val_loss: 0.7962 - val_acc: 0.7335\n"
     ]
    }
   ],
   "source": [
    "#<Train your model on the entire training set (50K samples)>\n",
    "model1 = model.fit(x_train, y_train_vec, batch_size=16, epochs=200, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the model on the test set (5 points)\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1767 - acc: 0.6224\n",
      "loss = 1.1767102479934692\n",
      "accuracy = 0.6223999857902527\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model performance (testing accuracy) on testing data.\n",
    "performance = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(performance[0]))\n",
    "print('accuracy = ' + str(performance[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building model with new structure (25 points)\n",
    "- In this section, you can build your model with adding new layers (e.g, BN layer or dropout layer, ...).\n",
    "- If you want to regularize a ```Conv/Dense layer```, you should place a ```Dropout layer``` before the ```Conv/Dense layer```.\n",
    "- You can try to compare their loss curve and testing accuracy and analyze your findings.\n",
    "- You need to try at lease two different model structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 626,378\n",
      "Trainable params: 626,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model2.add(Conv2D(64, (4, 4)))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(256, activation=\"relu\"))\n",
    "model2.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Nadam(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1250/1250 [==============================] - 8s 5ms/step - loss: 2.3539 - acc: 0.1159 - val_loss: 2.3075 - val_acc: 0.1156\n",
      "Epoch 2/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 2.2744 - acc: 0.1289 - val_loss: 2.1453 - val_acc: 0.2046\n",
      "Epoch 3/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9017 - acc: 0.2959 - val_loss: 1.6767 - val_acc: 0.3987\n",
      "Epoch 4/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6740 - acc: 0.3846 - val_loss: 1.5485 - val_acc: 0.4381\n",
      "Epoch 5/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5759 - acc: 0.4263 - val_loss: 1.4560 - val_acc: 0.4754\n",
      "Epoch 6/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4972 - acc: 0.4598 - val_loss: 1.4416 - val_acc: 0.4735\n",
      "Epoch 7/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4403 - acc: 0.4864 - val_loss: 1.3589 - val_acc: 0.5042\n",
      "Epoch 8/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3824 - acc: 0.5061 - val_loss: 1.2636 - val_acc: 0.5558\n",
      "Epoch 9/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3304 - acc: 0.5274 - val_loss: 1.2557 - val_acc: 0.5624\n",
      "Epoch 10/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2876 - acc: 0.5431 - val_loss: 1.3059 - val_acc: 0.5468\n",
      "Epoch 11/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2481 - acc: 0.5602 - val_loss: 1.1745 - val_acc: 0.5882\n",
      "Epoch 12/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2234 - acc: 0.5716 - val_loss: 1.1400 - val_acc: 0.6021\n",
      "Epoch 13/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1922 - acc: 0.5817 - val_loss: 1.1832 - val_acc: 0.5919\n",
      "Epoch 14/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1671 - acc: 0.5904 - val_loss: 1.1496 - val_acc: 0.5956\n",
      "Epoch 15/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1505 - acc: 0.6007 - val_loss: 1.1217 - val_acc: 0.6173\n",
      "Epoch 16/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1186 - acc: 0.6128 - val_loss: 1.1359 - val_acc: 0.6076\n",
      "Epoch 17/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1036 - acc: 0.6166 - val_loss: 1.0862 - val_acc: 0.6309\n",
      "Epoch 18/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0797 - acc: 0.6275 - val_loss: 1.0750 - val_acc: 0.6287\n",
      "Epoch 19/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0677 - acc: 0.6308 - val_loss: 1.1063 - val_acc: 0.6179\n",
      "Epoch 20/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0497 - acc: 0.6388 - val_loss: 1.1391 - val_acc: 0.6118\n",
      "Epoch 21/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0353 - acc: 0.6410 - val_loss: 1.0554 - val_acc: 0.6416\n",
      "Epoch 22/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0236 - acc: 0.6464 - val_loss: 1.1124 - val_acc: 0.6193\n",
      "Epoch 23/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0113 - acc: 0.6521 - val_loss: 1.0846 - val_acc: 0.6339\n",
      "Epoch 24/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9761 - acc: 0.6627 - val_loss: 1.0298 - val_acc: 0.6492\n",
      "Epoch 25/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9784 - acc: 0.6630 - val_loss: 1.0906 - val_acc: 0.6255\n",
      "Epoch 26/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9644 - acc: 0.6693 - val_loss: 1.0421 - val_acc: 0.6469\n",
      "Epoch 27/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9554 - acc: 0.6715 - val_loss: 1.0448 - val_acc: 0.6533\n",
      "Epoch 28/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9359 - acc: 0.6798 - val_loss: 1.0472 - val_acc: 0.6438\n",
      "Epoch 29/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9307 - acc: 0.6831 - val_loss: 1.0577 - val_acc: 0.6431\n",
      "Epoch 30/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9174 - acc: 0.6876 - val_loss: 1.1046 - val_acc: 0.6211\n",
      "Epoch 31/150\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9168 - acc: 0.6889 - val_loss: 1.0422 - val_acc: 0.6451\n",
      "Epoch 32/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8998 - acc: 0.6913 - val_loss: 1.1100 - val_acc: 0.6299\n",
      "Epoch 33/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8969 - acc: 0.6966 - val_loss: 1.0421 - val_acc: 0.6520\n",
      "Epoch 34/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8842 - acc: 0.7005 - val_loss: 1.0762 - val_acc: 0.6473\n",
      "Epoch 35/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8762 - acc: 0.7025 - val_loss: 1.0302 - val_acc: 0.6645\n",
      "Epoch 36/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8680 - acc: 0.7034 - val_loss: 1.0492 - val_acc: 0.6576\n",
      "Epoch 37/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8620 - acc: 0.7092 - val_loss: 1.0347 - val_acc: 0.6596\n",
      "Epoch 38/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8610 - acc: 0.7111 - val_loss: 1.0437 - val_acc: 0.6562\n",
      "Epoch 39/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8483 - acc: 0.7133 - val_loss: 1.0374 - val_acc: 0.6568\n",
      "Epoch 40/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8354 - acc: 0.7190 - val_loss: 1.0873 - val_acc: 0.6405\n",
      "Epoch 41/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8244 - acc: 0.7239 - val_loss: 1.0492 - val_acc: 0.6551\n",
      "Epoch 42/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8202 - acc: 0.7256 - val_loss: 1.0675 - val_acc: 0.6567\n",
      "Epoch 43/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8177 - acc: 0.7272 - val_loss: 1.0568 - val_acc: 0.6612\n",
      "Epoch 44/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8136 - acc: 0.7273 - val_loss: 1.0585 - val_acc: 0.6534\n",
      "Epoch 45/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8113 - acc: 0.7288 - val_loss: 1.1022 - val_acc: 0.6351\n",
      "Epoch 46/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7991 - acc: 0.7330 - val_loss: 1.0210 - val_acc: 0.6643\n",
      "Epoch 47/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7981 - acc: 0.7348 - val_loss: 1.0399 - val_acc: 0.6563\n",
      "Epoch 48/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7943 - acc: 0.7357 - val_loss: 1.1116 - val_acc: 0.6359\n",
      "Epoch 49/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7916 - acc: 0.7382 - val_loss: 1.0961 - val_acc: 0.6453\n",
      "Epoch 50/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7944 - acc: 0.7355 - val_loss: 1.0840 - val_acc: 0.6533\n",
      "Epoch 51/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7785 - acc: 0.7403 - val_loss: 1.0523 - val_acc: 0.6599\n",
      "Epoch 52/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7850 - acc: 0.7400 - val_loss: 1.0724 - val_acc: 0.6628\n",
      "Epoch 53/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7698 - acc: 0.7461 - val_loss: 1.0914 - val_acc: 0.6511\n",
      "Epoch 54/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7752 - acc: 0.7413 - val_loss: 1.0824 - val_acc: 0.6585\n",
      "Epoch 55/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7677 - acc: 0.7452 - val_loss: 1.0898 - val_acc: 0.6523\n",
      "Epoch 56/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7626 - acc: 0.7478 - val_loss: 1.0822 - val_acc: 0.6546\n",
      "Epoch 57/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7520 - acc: 0.7518 - val_loss: 1.1022 - val_acc: 0.6480\n",
      "Epoch 58/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7468 - acc: 0.7509 - val_loss: 1.0831 - val_acc: 0.6583\n",
      "Epoch 59/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7506 - acc: 0.7548 - val_loss: 1.0925 - val_acc: 0.6579\n",
      "Epoch 60/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7391 - acc: 0.7559 - val_loss: 1.1246 - val_acc: 0.6475\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7438 - acc: 0.7572 - val_loss: 1.0787 - val_acc: 0.6593\n",
      "Epoch 62/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7411 - acc: 0.7569 - val_loss: 1.0985 - val_acc: 0.6553\n",
      "Epoch 63/150\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7406 - acc: 0.7599 - val_loss: 1.0980 - val_acc: 0.6571\n",
      "Epoch 64/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7320 - acc: 0.7627 - val_loss: 1.1297 - val_acc: 0.6488\n",
      "Epoch 65/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7237 - acc: 0.7645 - val_loss: 1.1230 - val_acc: 0.6552\n",
      "Epoch 66/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7226 - acc: 0.7637 - val_loss: 1.1021 - val_acc: 0.6513\n",
      "Epoch 67/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7190 - acc: 0.7654 - val_loss: 1.0764 - val_acc: 0.6663\n",
      "Epoch 68/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7129 - acc: 0.7672 - val_loss: 1.1240 - val_acc: 0.6574\n",
      "Epoch 69/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7179 - acc: 0.7674 - val_loss: 1.1120 - val_acc: 0.6495\n",
      "Epoch 70/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7170 - acc: 0.7688 - val_loss: 1.1002 - val_acc: 0.6457\n",
      "Epoch 71/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7175 - acc: 0.7684 - val_loss: 1.1414 - val_acc: 0.6549\n",
      "Epoch 72/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7056 - acc: 0.7728 - val_loss: 1.0812 - val_acc: 0.6656\n",
      "Epoch 73/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7219 - acc: 0.7681 - val_loss: 1.1080 - val_acc: 0.6564\n",
      "Epoch 74/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7056 - acc: 0.7714 - val_loss: 1.1406 - val_acc: 0.6449\n",
      "Epoch 75/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7134 - acc: 0.7717 - val_loss: 1.0693 - val_acc: 0.6597\n",
      "Epoch 76/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6988 - acc: 0.7748 - val_loss: 1.2973 - val_acc: 0.6057\n",
      "Epoch 77/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7222 - acc: 0.7705 - val_loss: 1.1627 - val_acc: 0.6483\n",
      "Epoch 78/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6982 - acc: 0.7742 - val_loss: 1.1203 - val_acc: 0.6587\n",
      "Epoch 79/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6930 - acc: 0.7754 - val_loss: 1.0931 - val_acc: 0.6625\n",
      "Epoch 80/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6901 - acc: 0.7782 - val_loss: 1.1949 - val_acc: 0.6452\n",
      "Epoch 81/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6910 - acc: 0.7799 - val_loss: 1.0930 - val_acc: 0.6608\n",
      "Epoch 82/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7035 - acc: 0.7757 - val_loss: 1.1033 - val_acc: 0.6684\n",
      "Epoch 83/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6752 - acc: 0.7833 - val_loss: 1.0995 - val_acc: 0.6717\n",
      "Epoch 84/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6919 - acc: 0.7775 - val_loss: 1.1023 - val_acc: 0.6544\n",
      "Epoch 85/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7013 - acc: 0.7765 - val_loss: 1.2306 - val_acc: 0.6399\n",
      "Epoch 86/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7047 - acc: 0.7807 - val_loss: 1.1603 - val_acc: 0.6446\n",
      "Epoch 87/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6842 - acc: 0.7832 - val_loss: 1.1308 - val_acc: 0.6422\n",
      "Epoch 88/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6737 - acc: 0.7850 - val_loss: 1.1274 - val_acc: 0.6441\n",
      "Epoch 89/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6841 - acc: 0.7816 - val_loss: 1.1525 - val_acc: 0.6529\n",
      "Epoch 90/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6575 - acc: 0.7902 - val_loss: 1.1240 - val_acc: 0.6620\n",
      "Epoch 91/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6823 - acc: 0.7848 - val_loss: 1.1906 - val_acc: 0.6559\n",
      "Epoch 92/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6766 - acc: 0.7869 - val_loss: 1.2031 - val_acc: 0.6317\n",
      "Epoch 93/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6776 - acc: 0.7849 - val_loss: 1.1312 - val_acc: 0.6596\n",
      "Epoch 94/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6658 - acc: 0.7900 - val_loss: 1.1496 - val_acc: 0.6502\n",
      "Epoch 95/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6688 - acc: 0.7914 - val_loss: 1.1676 - val_acc: 0.6586\n",
      "Epoch 96/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7197 - acc: 0.7718 - val_loss: 1.1497 - val_acc: 0.6518\n",
      "Epoch 97/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6496 - acc: 0.7955 - val_loss: 1.1125 - val_acc: 0.6671\n",
      "Epoch 98/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6670 - acc: 0.7910 - val_loss: 1.1088 - val_acc: 0.6689\n",
      "Epoch 99/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6628 - acc: 0.7925 - val_loss: 1.1742 - val_acc: 0.6526\n",
      "Epoch 100/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6653 - acc: 0.7891 - val_loss: 1.2184 - val_acc: 0.6498\n",
      "Epoch 101/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6588 - acc: 0.7911 - val_loss: 1.2182 - val_acc: 0.6504\n",
      "Epoch 102/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6550 - acc: 0.7949 - val_loss: 1.1722 - val_acc: 0.6561\n",
      "Epoch 103/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6666 - acc: 0.7928 - val_loss: 1.1600 - val_acc: 0.6574\n",
      "Epoch 104/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6614 - acc: 0.7930 - val_loss: 1.2062 - val_acc: 0.6402\n",
      "Epoch 105/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6515 - acc: 0.7958 - val_loss: 1.2525 - val_acc: 0.6415\n",
      "Epoch 106/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6589 - acc: 0.7951 - val_loss: 1.1447 - val_acc: 0.6616\n",
      "Epoch 107/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6386 - acc: 0.7981 - val_loss: 1.1627 - val_acc: 0.6484\n",
      "Epoch 108/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6487 - acc: 0.7985 - val_loss: 1.1269 - val_acc: 0.6668\n",
      "Epoch 109/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6565 - acc: 0.7961 - val_loss: 1.1841 - val_acc: 0.6400\n",
      "Epoch 110/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6499 - acc: 0.7978 - val_loss: 1.1614 - val_acc: 0.6655\n",
      "Epoch 111/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6336 - acc: 0.8006 - val_loss: 1.2582 - val_acc: 0.6314\n",
      "Epoch 112/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6537 - acc: 0.7981 - val_loss: 1.1729 - val_acc: 0.6659\n",
      "Epoch 113/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6446 - acc: 0.8017 - val_loss: 1.1771 - val_acc: 0.6554\n",
      "Epoch 114/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6635 - acc: 0.7966 - val_loss: 1.1240 - val_acc: 0.6536\n",
      "Epoch 115/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6401 - acc: 0.8000 - val_loss: 1.1416 - val_acc: 0.6502\n",
      "Epoch 116/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6544 - acc: 0.7990 - val_loss: 1.1785 - val_acc: 0.6574\n",
      "Epoch 117/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6249 - acc: 0.8052 - val_loss: 1.1357 - val_acc: 0.6650\n",
      "Epoch 118/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6611 - acc: 0.7951 - val_loss: 1.1979 - val_acc: 0.6510\n",
      "Epoch 119/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6607 - acc: 0.7975 - val_loss: 1.1855 - val_acc: 0.6557\n",
      "Epoch 120/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6282 - acc: 0.8060 - val_loss: 1.2105 - val_acc: 0.6361\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6437 - acc: 0.8025 - val_loss: 1.1847 - val_acc: 0.6585\n",
      "Epoch 122/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6510 - acc: 0.8001 - val_loss: 1.1432 - val_acc: 0.6639\n",
      "Epoch 123/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6469 - acc: 0.8008 - val_loss: 1.1612 - val_acc: 0.6527\n",
      "Epoch 124/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6336 - acc: 0.8056 - val_loss: 1.1970 - val_acc: 0.6542\n",
      "Epoch 125/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6666 - acc: 0.7981 - val_loss: 1.2804 - val_acc: 0.6239\n",
      "Epoch 126/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6448 - acc: 0.8028 - val_loss: 1.2113 - val_acc: 0.6534\n",
      "Epoch 127/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6242 - acc: 0.8059 - val_loss: 1.2243 - val_acc: 0.6520\n",
      "Epoch 128/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6382 - acc: 0.8038 - val_loss: 1.2584 - val_acc: 0.6516\n",
      "Epoch 129/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6408 - acc: 0.8067 - val_loss: 1.2029 - val_acc: 0.6583\n",
      "Epoch 130/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6434 - acc: 0.8045 - val_loss: 1.1804 - val_acc: 0.6525\n",
      "Epoch 131/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6360 - acc: 0.8070 - val_loss: 1.1442 - val_acc: 0.6664\n",
      "Epoch 132/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6195 - acc: 0.8095 - val_loss: 1.2345 - val_acc: 0.6441\n",
      "Epoch 133/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6485 - acc: 0.8031 - val_loss: 1.1910 - val_acc: 0.6468\n",
      "Epoch 134/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6423 - acc: 0.8053 - val_loss: 1.2190 - val_acc: 0.6506\n",
      "Epoch 135/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6401 - acc: 0.8074 - val_loss: 1.1771 - val_acc: 0.6502\n",
      "Epoch 136/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6388 - acc: 0.8053 - val_loss: 1.2710 - val_acc: 0.6445\n",
      "Epoch 137/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6212 - acc: 0.8112 - val_loss: 1.2078 - val_acc: 0.6526\n",
      "Epoch 138/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6397 - acc: 0.8048 - val_loss: 1.2499 - val_acc: 0.6445\n",
      "Epoch 139/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6523 - acc: 0.8044 - val_loss: 1.2341 - val_acc: 0.6466\n",
      "Epoch 140/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6404 - acc: 0.8062 - val_loss: 1.1972 - val_acc: 0.6719\n",
      "Epoch 141/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6452 - acc: 0.8066 - val_loss: 1.1938 - val_acc: 0.6609\n",
      "Epoch 142/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6476 - acc: 0.8061 - val_loss: 1.2454 - val_acc: 0.6296\n",
      "Epoch 143/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6435 - acc: 0.8044 - val_loss: 1.1978 - val_acc: 0.6623\n",
      "Epoch 144/150\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6029 - acc: 0.8136 - val_loss: 1.2860 - val_acc: 0.6296\n",
      "Epoch 145/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6462 - acc: 0.8057 - val_loss: 1.3169 - val_acc: 0.6664\n",
      "Epoch 146/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6420 - acc: 0.8061 - val_loss: 1.2821 - val_acc: 0.6433\n",
      "Epoch 147/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6435 - acc: 0.8088 - val_loss: 1.4107 - val_acc: 0.6178s - loss: 0.6461 - acc: \n",
      "Epoch 148/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6263 - acc: 0.8084 - val_loss: 1.2087 - val_acc: 0.6576\n",
      "Epoch 149/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6388 - acc: 0.8107 - val_loss: 1.2120 - val_acc: 0.6722\n",
      "Epoch 150/150\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6341 - acc: 0.8124 - val_loss: 1.1991 - val_acc: 0.6622\n"
     ]
    }
   ],
   "source": [
    "history1 = model2.fit(x_tr, y_tr, batch_size=32, epochs=150, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Nadam(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.9088 - acc: 0.7419 - val_loss: 1.0484 - val_acc: 0.6510\n",
      "Epoch 2/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8731 - acc: 0.7415 - val_loss: 0.9358 - val_acc: 0.6873\n",
      "Epoch 3/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8420 - acc: 0.7496 - val_loss: 0.9140 - val_acc: 0.6932\n",
      "Epoch 4/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8113 - acc: 0.7571 - val_loss: 0.9047 - val_acc: 0.6980\n",
      "Epoch 5/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8147 - acc: 0.7560 - val_loss: 0.9159 - val_acc: 0.6934\n",
      "Epoch 6/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8204 - acc: 0.7522 - val_loss: 0.8058 - val_acc: 0.7349\n",
      "Epoch 7/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8019 - acc: 0.7602 - val_loss: 0.8004 - val_acc: 0.7287\n",
      "Epoch 8/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7771 - acc: 0.7642 - val_loss: 0.8111 - val_acc: 0.7325\n",
      "Epoch 9/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7684 - acc: 0.7682 - val_loss: 0.7797 - val_acc: 0.7368\n",
      "Epoch 10/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7766 - acc: 0.7629 - val_loss: 0.7267 - val_acc: 0.7545\n",
      "Epoch 11/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7588 - acc: 0.7695 - val_loss: 0.7051 - val_acc: 0.7583\n",
      "Epoch 12/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7822 - acc: 0.7637 - val_loss: 0.7163 - val_acc: 0.7547\n",
      "Epoch 13/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7653 - acc: 0.7696 - val_loss: 0.7872 - val_acc: 0.7427\n",
      "Epoch 14/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7753 - acc: 0.7645 - val_loss: 0.8134 - val_acc: 0.7306\n",
      "Epoch 15/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7880 - acc: 0.7629 - val_loss: 0.7555 - val_acc: 0.7486\n",
      "Epoch 16/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7717 - acc: 0.7668 - val_loss: 0.7086 - val_acc: 0.7682\n",
      "Epoch 17/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8022 - acc: 0.7596 - val_loss: 0.9105 - val_acc: 0.6949\n",
      "Epoch 18/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7525 - acc: 0.7721 - val_loss: 0.6440 - val_acc: 0.7861\n",
      "Epoch 19/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7689 - acc: 0.7683 - val_loss: 0.6759 - val_acc: 0.7773\n",
      "Epoch 20/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7491 - acc: 0.7740 - val_loss: 0.6213 - val_acc: 0.7937\n",
      "Epoch 21/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7791 - acc: 0.7655 - val_loss: 0.6908 - val_acc: 0.7743\n",
      "Epoch 22/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7845 - acc: 0.7667 - val_loss: 0.7206 - val_acc: 0.7560\n",
      "Epoch 23/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7695 - acc: 0.7695 - val_loss: 0.6495 - val_acc: 0.7905\n",
      "Epoch 24/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7702 - acc: 0.7672 - val_loss: 0.6753 - val_acc: 0.7727\n",
      "Epoch 25/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7715 - acc: 0.7704 - val_loss: 0.6818 - val_acc: 0.7695\n",
      "Epoch 26/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7587 - acc: 0.7729 - val_loss: 0.6092 - val_acc: 0.8005\n",
      "Epoch 27/150\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7414 - acc: 0.7736 - val_loss: 0.6251 - val_acc: 0.7937\n",
      "Epoch 28/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7935 - acc: 0.7604 - val_loss: 0.6598 - val_acc: 0.7735\n",
      "Epoch 29/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7688 - acc: 0.7664 - val_loss: 0.5946 - val_acc: 0.8028\n",
      "Epoch 30/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7602 - acc: 0.7715 - val_loss: 0.5975 - val_acc: 0.8040\n",
      "Epoch 31/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7439 - acc: 0.7746 - val_loss: 0.6283 - val_acc: 0.7922\n",
      "Epoch 32/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7464 - acc: 0.7754 - val_loss: 0.6360 - val_acc: 0.7929\n",
      "Epoch 33/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7864 - acc: 0.7657 - val_loss: 0.6684 - val_acc: 0.7855\n",
      "Epoch 34/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7554 - acc: 0.7723 - val_loss: 0.7931 - val_acc: 0.7312\n",
      "Epoch 35/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7653 - acc: 0.7698 - val_loss: 0.5735 - val_acc: 0.8111\n",
      "Epoch 36/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7673 - acc: 0.7680 - val_loss: 0.6045 - val_acc: 0.7997\n",
      "Epoch 37/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7593 - acc: 0.7725 - val_loss: 0.6357 - val_acc: 0.7917\n",
      "Epoch 38/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7613 - acc: 0.7707 - val_loss: 0.6640 - val_acc: 0.7751\n",
      "Epoch 39/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7562 - acc: 0.7728 - val_loss: 0.5626 - val_acc: 0.8096\n",
      "Epoch 40/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7504 - acc: 0.7741 - val_loss: 0.5794 - val_acc: 0.8112\n",
      "Epoch 41/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7580 - acc: 0.7752 - val_loss: 0.6636 - val_acc: 0.7831\n",
      "Epoch 42/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7515 - acc: 0.7754 - val_loss: 0.5745 - val_acc: 0.8113\n",
      "Epoch 43/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7499 - acc: 0.7778 - val_loss: 0.6470 - val_acc: 0.7800\n",
      "Epoch 44/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7517 - acc: 0.7745 - val_loss: 0.8878 - val_acc: 0.7140\n",
      "Epoch 45/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7688 - acc: 0.7691 - val_loss: 0.5640 - val_acc: 0.8185\n",
      "Epoch 46/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7452 - acc: 0.7763 - val_loss: 0.6064 - val_acc: 0.7999\n",
      "Epoch 47/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7524 - acc: 0.7738 - val_loss: 0.5872 - val_acc: 0.8178\n",
      "Epoch 48/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7681 - acc: 0.7719 - val_loss: 0.5792 - val_acc: 0.8124\n",
      "Epoch 49/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8469 - acc: 0.7506 - val_loss: 0.6462 - val_acc: 0.7904\n",
      "Epoch 50/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7420 - acc: 0.7799 - val_loss: 0.5196 - val_acc: 0.8282\n",
      "Epoch 51/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7499 - acc: 0.7772 - val_loss: 0.5835 - val_acc: 0.8138\n",
      "Epoch 52/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7644 - acc: 0.7759 - val_loss: 0.7295 - val_acc: 0.7620\n",
      "Epoch 53/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7677 - acc: 0.7730 - val_loss: 0.6378 - val_acc: 0.7930\n",
      "Epoch 54/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7453 - acc: 0.7792 - val_loss: 0.5114 - val_acc: 0.8366\n",
      "Epoch 55/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7762 - acc: 0.7734 - val_loss: 0.5745 - val_acc: 0.8157\n",
      "Epoch 56/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7464 - acc: 0.7795 - val_loss: 0.6176 - val_acc: 0.7980\n",
      "Epoch 57/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7725 - acc: 0.7720 - val_loss: 0.5737 - val_acc: 0.8117\n",
      "Epoch 58/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7582 - acc: 0.7740 - val_loss: 0.5622 - val_acc: 0.8211\n",
      "Epoch 59/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7537 - acc: 0.7759 - val_loss: 0.5625 - val_acc: 0.8189\n",
      "Epoch 60/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7637 - acc: 0.7705 - val_loss: 0.6032 - val_acc: 0.8089\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7508 - acc: 0.7761 - val_loss: 0.6675 - val_acc: 0.7837\n",
      "Epoch 62/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7623 - acc: 0.7757 - val_loss: 0.5465 - val_acc: 0.8256\n",
      "Epoch 63/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7645 - acc: 0.7721 - val_loss: 0.5678 - val_acc: 0.8196\n",
      "Epoch 64/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7893 - acc: 0.7640 - val_loss: 0.5704 - val_acc: 0.8105\n",
      "Epoch 65/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7626 - acc: 0.7738 - val_loss: 0.5612 - val_acc: 0.8199\n",
      "Epoch 66/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7507 - acc: 0.7770 - val_loss: 0.5428 - val_acc: 0.8216\n",
      "Epoch 67/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7613 - acc: 0.7740 - val_loss: 0.6016 - val_acc: 0.8090\n",
      "Epoch 68/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7430 - acc: 0.7822 - val_loss: 0.5023 - val_acc: 0.8353\n",
      "Epoch 69/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7446 - acc: 0.7777 - val_loss: 0.5539 - val_acc: 0.8237\n",
      "Epoch 70/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7655 - acc: 0.7747 - val_loss: 0.5986 - val_acc: 0.8139\n",
      "Epoch 71/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7399 - acc: 0.7795 - val_loss: 0.4964 - val_acc: 0.8458\n",
      "Epoch 72/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7678 - acc: 0.7742 - val_loss: 0.5893 - val_acc: 0.8028\n",
      "Epoch 73/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7591 - acc: 0.7721 - val_loss: 0.5724 - val_acc: 0.8124\n",
      "Epoch 74/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7543 - acc: 0.7765 - val_loss: 0.5822 - val_acc: 0.8062\n",
      "Epoch 75/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7729 - acc: 0.7723 - val_loss: 0.8045 - val_acc: 0.7526\n",
      "Epoch 76/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7654 - acc: 0.7735 - val_loss: 0.5699 - val_acc: 0.8154\n",
      "Epoch 77/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7656 - acc: 0.7748 - val_loss: 0.5714 - val_acc: 0.8165\n",
      "Epoch 78/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7645 - acc: 0.7772 - val_loss: 0.6144 - val_acc: 0.8052\n",
      "Epoch 79/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8046 - acc: 0.7634 - val_loss: 0.5796 - val_acc: 0.8046\n",
      "Epoch 80/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7734 - acc: 0.7749 - val_loss: 0.5722 - val_acc: 0.8237\n",
      "Epoch 81/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7313 - acc: 0.7855 - val_loss: 0.5536 - val_acc: 0.8182\n",
      "Epoch 82/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8313 - acc: 0.7537 - val_loss: 0.6075 - val_acc: 0.8075\n",
      "Epoch 83/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7525 - acc: 0.7793 - val_loss: 0.6104 - val_acc: 0.7995\n",
      "Epoch 84/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8018 - acc: 0.7652 - val_loss: 0.8307 - val_acc: 0.7222\n",
      "Epoch 85/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7802 - acc: 0.7720 - val_loss: 0.6310 - val_acc: 0.7933\n",
      "Epoch 86/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7899 - acc: 0.7678 - val_loss: 0.5274 - val_acc: 0.8328\n",
      "Epoch 87/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7934 - acc: 0.7675 - val_loss: 0.5446 - val_acc: 0.8291\n",
      "Epoch 88/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7542 - acc: 0.7788 - val_loss: 0.4898 - val_acc: 0.8469\n",
      "Epoch 89/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7821 - acc: 0.7734 - val_loss: 0.5286 - val_acc: 0.8280\n",
      "Epoch 90/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8180 - acc: 0.7622 - val_loss: 0.5352 - val_acc: 0.8297\n",
      "Epoch 91/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0394 - acc: 0.6825 - val_loss: 0.5366 - val_acc: 0.8246\n",
      "Epoch 92/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7759 - acc: 0.7721 - val_loss: 0.5190 - val_acc: 0.8361\n",
      "Epoch 93/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8271 - acc: 0.7567 - val_loss: 0.5951 - val_acc: 0.8068\n",
      "Epoch 94/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7678 - acc: 0.7733 - val_loss: 0.5824 - val_acc: 0.8101\n",
      "Epoch 95/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8624 - acc: 0.7449 - val_loss: 0.5128 - val_acc: 0.8315\n",
      "Epoch 96/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7964 - acc: 0.7674 - val_loss: 0.4817 - val_acc: 0.8445\n",
      "Epoch 97/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1177 - acc: 0.6498 - val_loss: 0.7322 - val_acc: 0.7598\n",
      "Epoch 98/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8685 - acc: 0.7468 - val_loss: 0.5052 - val_acc: 0.8395\n",
      "Epoch 99/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7944 - acc: 0.7676 - val_loss: 0.5325 - val_acc: 0.8217\n",
      "Epoch 100/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7981 - acc: 0.7656 - val_loss: 0.4888 - val_acc: 0.8463\n",
      "Epoch 101/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8101 - acc: 0.7634 - val_loss: 0.5187 - val_acc: 0.8406\n",
      "Epoch 102/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8685 - acc: 0.7464 - val_loss: 0.6439 - val_acc: 0.7972\n",
      "Epoch 103/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8369 - acc: 0.7545 - val_loss: 0.5404 - val_acc: 0.8166\n",
      "Epoch 104/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8071 - acc: 0.7646 - val_loss: 0.6561 - val_acc: 0.7888\n",
      "Epoch 105/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8157 - acc: 0.7599 - val_loss: 0.5067 - val_acc: 0.8410\n",
      "Epoch 106/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7951 - acc: 0.7661 - val_loss: 0.5223 - val_acc: 0.8394\n",
      "Epoch 107/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8211 - acc: 0.7594 - val_loss: 0.5247 - val_acc: 0.8353\n",
      "Epoch 108/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8466 - acc: 0.7491 - val_loss: 0.4819 - val_acc: 0.8428\n",
      "Epoch 109/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8293 - acc: 0.7568 - val_loss: 0.5634 - val_acc: 0.8247\n",
      "Epoch 110/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8249 - acc: 0.7588 - val_loss: 0.5723 - val_acc: 0.8169\n",
      "Epoch 111/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8299 - acc: 0.7570 - val_loss: 0.5389 - val_acc: 0.8370\n",
      "Epoch 112/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8115 - acc: 0.7642 - val_loss: 0.6639 - val_acc: 0.7867\n",
      "Epoch 113/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8365 - acc: 0.7606 - val_loss: 0.6432 - val_acc: 0.8047\n",
      "Epoch 114/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7941 - acc: 0.7678 - val_loss: 0.5308 - val_acc: 0.8299\n",
      "Epoch 115/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8142 - acc: 0.7650 - val_loss: 0.5672 - val_acc: 0.8245\n",
      "Epoch 116/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8118 - acc: 0.7619 - val_loss: 0.5825 - val_acc: 0.8065\n",
      "Epoch 117/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8113 - acc: 0.7654 - val_loss: 0.4934 - val_acc: 0.8446\n",
      "Epoch 118/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8327 - acc: 0.7573 - val_loss: 0.5495 - val_acc: 0.8237\n",
      "Epoch 119/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8234 - acc: 0.7582 - val_loss: 1.2137 - val_acc: 0.5766\n",
      "Epoch 120/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8082 - acc: 0.7643 - val_loss: 0.4856 - val_acc: 0.8476\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8623 - acc: 0.7494 - val_loss: 0.5743 - val_acc: 0.8140\n",
      "Epoch 122/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8104 - acc: 0.7656 - val_loss: 0.5792 - val_acc: 0.8108\n",
      "Epoch 123/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8869 - acc: 0.7417 - val_loss: 0.5315 - val_acc: 0.8266\n",
      "Epoch 124/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9079 - acc: 0.7405 - val_loss: 0.6406 - val_acc: 0.7920\n",
      "Epoch 125/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8454 - acc: 0.7537 - val_loss: 0.7183 - val_acc: 0.7588\n",
      "Epoch 126/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8277 - acc: 0.7572 - val_loss: 0.5888 - val_acc: 0.8157\n",
      "Epoch 127/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8534 - acc: 0.7522 - val_loss: 0.5324 - val_acc: 0.8356\n",
      "Epoch 128/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8206 - acc: 0.7643 - val_loss: 0.5803 - val_acc: 0.8136\n",
      "Epoch 129/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8500 - acc: 0.7503 - val_loss: 0.5984 - val_acc: 0.8051\n",
      "Epoch 130/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8186 - acc: 0.7620 - val_loss: 0.5755 - val_acc: 0.8165\n",
      "Epoch 131/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8587 - acc: 0.7500 - val_loss: 0.5752 - val_acc: 0.8210\n",
      "Epoch 132/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8248 - acc: 0.7614 - val_loss: 0.5758 - val_acc: 0.8181\n",
      "Epoch 133/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8468 - acc: 0.7538 - val_loss: 0.6001 - val_acc: 0.8104\n",
      "Epoch 134/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8862 - acc: 0.7406 - val_loss: 0.5589 - val_acc: 0.8182\n",
      "Epoch 135/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8569 - acc: 0.7484 - val_loss: 0.5491 - val_acc: 0.8262\n",
      "Epoch 136/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8588 - acc: 0.7499 - val_loss: 0.7244 - val_acc: 0.7576\n",
      "Epoch 137/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8364 - acc: 0.7562 - val_loss: 0.5171 - val_acc: 0.8346\n",
      "Epoch 138/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8563 - acc: 0.7552 - val_loss: 0.7431 - val_acc: 0.7593\n",
      "Epoch 139/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8415 - acc: 0.7561 - val_loss: 0.5939 - val_acc: 0.8126\n",
      "Epoch 140/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8482 - acc: 0.7544 - val_loss: 0.7614 - val_acc: 0.7509\n",
      "Epoch 141/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9146 - acc: 0.7339 - val_loss: 0.8229 - val_acc: 0.7208\n",
      "Epoch 142/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8775 - acc: 0.7440 - val_loss: 0.6028 - val_acc: 0.8062\n",
      "Epoch 143/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8796 - acc: 0.7410 - val_loss: 0.6614 - val_acc: 0.7915\n",
      "Epoch 144/150\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8729 - acc: 0.7453 - val_loss: 0.5731 - val_acc: 0.8195\n",
      "Epoch 145/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9518 - acc: 0.7191 - val_loss: 0.5754 - val_acc: 0.8279\n",
      "Epoch 146/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8917 - acc: 0.7359 - val_loss: 0.7005 - val_acc: 0.7788\n",
      "Epoch 147/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8606 - acc: 0.7477 - val_loss: 0.6727 - val_acc: 0.7772\n",
      "Epoch 148/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8746 - acc: 0.7457 - val_loss: 0.5551 - val_acc: 0.8259\n",
      "Epoch 149/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8758 - acc: 0.7461 - val_loss: 0.8300 - val_acc: 0.7283\n",
      "Epoch 150/150\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8830 - acc: 0.7392 - val_loss: 0.6362 - val_acc: 0.8003\n"
     ]
    }
   ],
   "source": [
    "history1 = model2.fit(x_train, y_train_vec, batch_size=32, epochs=150, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.2757 - acc: 0.6397\n",
      "loss = 1.2757397890090942\n",
      "accuracy = 0.6396999955177307\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model performance (testing accuracy) on testing data.\n",
    "performance = model2.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(performance[0]))\n",
    "print('accuracy = ' + str(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 626,378\n",
      "Trainable params: 626,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Conv2D(64, (4, 4)))\n",
    "model3.add(Activation(\"relu\"))\n",
    "model3.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(256, activation=\"relu\"))\n",
    "model3.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Adamax(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 6.7213 - acc: 0.1133 - val_loss: 2.2952 - val_acc: 0.1093\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.2935 - acc: 0.1217 - val_loss: 2.2820 - val_acc: 0.1170\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.2672 - acc: 0.1319 - val_loss: 2.2696 - val_acc: 0.1261\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.2325 - acc: 0.1550 - val_loss: 2.2180 - val_acc: 0.1589\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.1974 - acc: 0.1770 - val_loss: 2.2028 - val_acc: 0.1672\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.1532 - acc: 0.1909 - val_loss: 2.1545 - val_acc: 0.1819\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.1041 - acc: 0.2048 - val_loss: 2.1785 - val_acc: 0.1639\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.0671 - acc: 0.2127 - val_loss: 2.0943 - val_acc: 0.1934\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.0128 - acc: 0.2371 - val_loss: 2.0309 - val_acc: 0.2205\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.9682 - acc: 0.2549 - val_loss: 1.9801 - val_acc: 0.2430\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.9328 - acc: 0.2676 - val_loss: 1.9441 - val_acc: 0.2564\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.9057 - acc: 0.2748 - val_loss: 1.9524 - val_acc: 0.2462\n",
      "Epoch 13/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.8782 - acc: 0.2846 - val_loss: 1.8487 - val_acc: 0.3003\n",
      "Epoch 14/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.8461 - acc: 0.2995 - val_loss: 1.7915 - val_acc: 0.3211\n",
      "Epoch 15/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.8250 - acc: 0.3072 - val_loss: 1.7537 - val_acc: 0.3333\n",
      "Epoch 16/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.7954 - acc: 0.3215 - val_loss: 1.7542 - val_acc: 0.3388\n",
      "Epoch 17/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.7727 - acc: 0.3313 - val_loss: 1.7256 - val_acc: 0.3497\n",
      "Epoch 18/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.7540 - acc: 0.3412 - val_loss: 1.7122 - val_acc: 0.3567\n",
      "Epoch 19/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.7301 - acc: 0.3551 - val_loss: 1.6681 - val_acc: 0.3847\n",
      "Epoch 20/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.6828 - acc: 0.3733 - val_loss: 1.6250 - val_acc: 0.4065\n",
      "Epoch 21/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.6306 - acc: 0.3967 - val_loss: 1.6072 - val_acc: 0.4051\n",
      "Epoch 22/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.6008 - acc: 0.4103 - val_loss: 1.5144 - val_acc: 0.4504\n",
      "Epoch 23/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.5654 - acc: 0.4263 - val_loss: 1.4725 - val_acc: 0.4605\n",
      "Epoch 24/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.5397 - acc: 0.4388 - val_loss: 1.4617 - val_acc: 0.4741\n",
      "Epoch 25/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.4952 - acc: 0.4602 - val_loss: 1.4499 - val_acc: 0.4764\n",
      "Epoch 26/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.4572 - acc: 0.4717 - val_loss: 1.3627 - val_acc: 0.5122\n",
      "Epoch 27/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.4279 - acc: 0.4876 - val_loss: 1.3354 - val_acc: 0.5309\n",
      "Epoch 28/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.3801 - acc: 0.5046 - val_loss: 1.2861 - val_acc: 0.5374\n",
      "Epoch 29/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.3390 - acc: 0.5211 - val_loss: 1.2810 - val_acc: 0.5466\n",
      "Epoch 30/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.3068 - acc: 0.5352 - val_loss: 1.2275 - val_acc: 0.5725\n",
      "Epoch 31/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.2601 - acc: 0.5526 - val_loss: 1.1631 - val_acc: 0.6008\n",
      "Epoch 32/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.2255 - acc: 0.5668 - val_loss: 1.1404 - val_acc: 0.6094\n",
      "Epoch 33/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.1967 - acc: 0.5773 - val_loss: 1.1384 - val_acc: 0.6101\n",
      "Epoch 34/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.1647 - acc: 0.5881 - val_loss: 1.1026 - val_acc: 0.6234\n",
      "Epoch 35/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.1445 - acc: 0.5980 - val_loss: 1.0996 - val_acc: 0.6204\n",
      "Epoch 36/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.1211 - acc: 0.6043 - val_loss: 1.0435 - val_acc: 0.6452\n",
      "Epoch 37/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.0860 - acc: 0.6197 - val_loss: 1.0387 - val_acc: 0.6405\n",
      "Epoch 38/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.0598 - acc: 0.6259 - val_loss: 1.0162 - val_acc: 0.6524\n",
      "Epoch 39/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.0353 - acc: 0.6361 - val_loss: 0.9878 - val_acc: 0.6641\n",
      "Epoch 40/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.0133 - acc: 0.6452 - val_loss: 1.0023 - val_acc: 0.6559\n",
      "Epoch 41/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.9915 - acc: 0.6506 - val_loss: 0.9769 - val_acc: 0.6614\n",
      "Epoch 42/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.9753 - acc: 0.6575 - val_loss: 0.9499 - val_acc: 0.6700\n",
      "Epoch 43/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.9512 - acc: 0.6637 - val_loss: 0.9530 - val_acc: 0.6705\n",
      "Epoch 44/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.9340 - acc: 0.6721 - val_loss: 0.9189 - val_acc: 0.6837\n",
      "Epoch 45/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.9131 - acc: 0.6762 - val_loss: 0.9114 - val_acc: 0.6859\n",
      "Epoch 46/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.8953 - acc: 0.6876 - val_loss: 0.8984 - val_acc: 0.6853\n",
      "Epoch 47/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.8741 - acc: 0.6926 - val_loss: 0.8944 - val_acc: 0.6891\n",
      "Epoch 48/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.8558 - acc: 0.6959 - val_loss: 0.8738 - val_acc: 0.6954\n",
      "Epoch 49/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.8352 - acc: 0.7080 - val_loss: 0.8546 - val_acc: 0.7047\n",
      "Epoch 50/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.8277 - acc: 0.7119 - val_loss: 0.8684 - val_acc: 0.6976\n"
     ]
    }
   ],
   "source": [
    "history2 = model3.fit(x_tr, y_tr, batch_size=256, epochs=50, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss = 'categorical_crossentropy', optimizer = optimizers.Adamax(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.8892 - acc: 0.6933 - val_loss: 0.7887 - val_acc: 0.7344\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.8408 - acc: 0.7073 - val_loss: 0.7634 - val_acc: 0.7407\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.8177 - acc: 0.7155 - val_loss: 0.7118 - val_acc: 0.7575\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7955 - acc: 0.7221 - val_loss: 0.6833 - val_acc: 0.7720\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7831 - acc: 0.7281 - val_loss: 0.6576 - val_acc: 0.7756\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7612 - acc: 0.7322 - val_loss: 0.6248 - val_acc: 0.7942\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7492 - acc: 0.7383 - val_loss: 0.6112 - val_acc: 0.8013\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7358 - acc: 0.7415 - val_loss: 0.5971 - val_acc: 0.8079\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.7191 - acc: 0.7490 - val_loss: 0.5800 - val_acc: 0.8129\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.7099 - acc: 0.7519 - val_loss: 0.5486 - val_acc: 0.8261\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6908 - acc: 0.7572 - val_loss: 0.5235 - val_acc: 0.8352\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6863 - acc: 0.7587 - val_loss: 0.4956 - val_acc: 0.8493\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6690 - acc: 0.7671 - val_loss: 0.4927 - val_acc: 0.8555\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6597 - acc: 0.7662 - val_loss: 0.4632 - val_acc: 0.8544\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6534 - acc: 0.7715 - val_loss: 0.4390 - val_acc: 0.8677\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6308 - acc: 0.7766 - val_loss: 0.4198 - val_acc: 0.8765\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6241 - acc: 0.7801 - val_loss: 0.3907 - val_acc: 0.8905\n",
      "Epoch 18/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6121 - acc: 0.7824 - val_loss: 0.3878 - val_acc: 0.8922\n",
      "Epoch 19/50\n",
      "196/196 [==============================] - 2s 9ms/step - loss: 0.6026 - acc: 0.7878 - val_loss: 0.4035 - val_acc: 0.8795\n",
      "Epoch 20/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5898 - acc: 0.7899 - val_loss: 0.3666 - val_acc: 0.8972\n",
      "Epoch 21/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5787 - acc: 0.7959 - val_loss: 0.3477 - val_acc: 0.9013\n",
      "Epoch 22/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5711 - acc: 0.7973 - val_loss: 0.3404 - val_acc: 0.9040\n",
      "Epoch 23/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5559 - acc: 0.8028 - val_loss: 0.3117 - val_acc: 0.9201\n",
      "Epoch 24/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5552 - acc: 0.8049 - val_loss: 0.3267 - val_acc: 0.9149\n",
      "Epoch 25/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5450 - acc: 0.8069 - val_loss: 0.3067 - val_acc: 0.9221\n",
      "Epoch 26/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5450 - acc: 0.8080 - val_loss: 0.2997 - val_acc: 0.9221\n",
      "Epoch 27/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5309 - acc: 0.8109 - val_loss: 0.2954 - val_acc: 0.9251\n",
      "Epoch 28/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5179 - acc: 0.8141 - val_loss: 0.2669 - val_acc: 0.9333\n",
      "Epoch 29/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5136 - acc: 0.8170 - val_loss: 0.2559 - val_acc: 0.9385\n",
      "Epoch 30/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5049 - acc: 0.8226 - val_loss: 0.2596 - val_acc: 0.9371\n",
      "Epoch 31/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4899 - acc: 0.8262 - val_loss: 0.2378 - val_acc: 0.9416\n",
      "Epoch 32/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4909 - acc: 0.8264 - val_loss: 0.2348 - val_acc: 0.9405\n",
      "Epoch 33/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4867 - acc: 0.8274 - val_loss: 0.2327 - val_acc: 0.9448\n",
      "Epoch 34/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4807 - acc: 0.8284 - val_loss: 0.2162 - val_acc: 0.9505\n",
      "Epoch 35/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4701 - acc: 0.8353 - val_loss: 0.2204 - val_acc: 0.9484\n",
      "Epoch 36/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4590 - acc: 0.8378 - val_loss: 0.1987 - val_acc: 0.9561\n",
      "Epoch 37/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4565 - acc: 0.8376 - val_loss: 0.1907 - val_acc: 0.9575\n",
      "Epoch 38/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4565 - acc: 0.8374 - val_loss: 0.1972 - val_acc: 0.9563\n",
      "Epoch 39/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4448 - acc: 0.8423 - val_loss: 0.1793 - val_acc: 0.9625\n",
      "Epoch 40/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4411 - acc: 0.8428 - val_loss: 0.1818 - val_acc: 0.9660\n",
      "Epoch 41/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4385 - acc: 0.8454 - val_loss: 0.1945 - val_acc: 0.9520\n",
      "Epoch 42/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4254 - acc: 0.8486 - val_loss: 0.1693 - val_acc: 0.9660\n",
      "Epoch 43/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4263 - acc: 0.8501 - val_loss: 0.1693 - val_acc: 0.9622\n",
      "Epoch 44/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4149 - acc: 0.8504 - val_loss: 0.1469 - val_acc: 0.9738\n",
      "Epoch 45/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4116 - acc: 0.8538 - val_loss: 0.1679 - val_acc: 0.9696\n",
      "Epoch 46/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4106 - acc: 0.8552 - val_loss: 0.1434 - val_acc: 0.9757\n",
      "Epoch 47/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4078 - acc: 0.8567 - val_loss: 0.1487 - val_acc: 0.9713\n",
      "Epoch 48/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.4043 - acc: 0.8573 - val_loss: 0.1495 - val_acc: 0.9717\n",
      "Epoch 49/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.3937 - acc: 0.8590 - val_loss: 0.1335 - val_acc: 0.9752\n",
      "Epoch 50/50\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.3860 - acc: 0.8638 - val_loss: 0.1241 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "history2 = model3.fit(x_train, y_train_vec, batch_size=256, epochs=50, validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7657 - acc: 0.7610\n",
      "loss = 0.765708327293396\n",
      "accuracy = 0.7609999775886536\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model performance (testing accuracy) on testing data.\n",
    "performance = model3.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(performance[0]))\n",
    "print('accuracy = ' + str(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
