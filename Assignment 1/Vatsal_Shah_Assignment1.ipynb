{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ievsTNxbIh9"
      },
      "source": [
        "# HM1: Logistic Regression.\n",
        "\n",
        "### Name: Vatsal Shah\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuuhKkQNbIiT"
      },
      "source": [
        "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
        "\n",
        "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_idlwnvpbIiV"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "import numpy as np\n",
        "import sklearn \n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkpzc6CbIiY"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6inbZFk-bIiZ"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5CTDXUyUbIia"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WdPFipbIib"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDl0rQ5bIid",
        "outputId": "9b7bcdc0-4635-493c-aa03-12382ea8fc3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
        "# You need to get rid of the ID number feature.\n",
        "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "data = data.drop(\"id\", axis=1)\n",
        "data = data.drop(\"Unnamed: 32\", axis=1)\n",
        "le.fit(data.diagnosis)\n",
        "le.classes_ = le.classes_[::-1]\n",
        "data.diagnosis = le.transform(data.diagnosis)\n",
        "data.diagnosis = data[\"diagnosis\"].replace(0, -1)"
      ],
      "metadata": {
        "id": "l4G7cDJi3KqD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"diagnosis\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eZpII_I-D_M",
        "outputId": "5f7a679a-2609-4d6a-9f62-7ee392aab667"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4VV-arzEOla",
        "outputId": "b9ac5a94-573b-48e3-85a6-08033e1507e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "Ck2NiovrY_Ah",
        "outputId": "73a1f7bc-5373-4c78-b26d-1e5232fd123c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-03842a8f-159d-43bb-b2bb-bf060a088963\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>-1</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>-1</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>1</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03842a8f-159d-43bb-b2bb-bf060a088963')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03842a8f-159d-43bb-b2bb-bf060a088963 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03842a8f-159d-43bb-b2bb-bf060a088963');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0           -1        17.99  ...          0.4601                  0.11890\n",
              "1           -1        20.57  ...          0.2750                  0.08902\n",
              "2           -1        19.69  ...          0.3613                  0.08758\n",
              "3           -1        11.42  ...          0.6638                  0.17300\n",
              "4           -1        20.29  ...          0.2364                  0.07678\n",
              "..         ...          ...  ...             ...                      ...\n",
              "564         -1        21.56  ...          0.2060                  0.07115\n",
              "565         -1        20.13  ...          0.2572                  0.06637\n",
              "566         -1        16.60  ...          0.2218                  0.07820\n",
              "567         -1        20.60  ...          0.4087                  0.12400\n",
              "568          1         7.76  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X89p4bbbIif"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_kdGBG13bIig"
      },
      "outputs": [],
      "source": [
        "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
        "x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,1:31], data.diagnosis, train_size=0.8, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilyp60HSbIii"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdKbu0pVbIij"
      },
      "source": [
        "Use the standardization to trainsform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkB9ouP4bIik",
        "outputId": "a61188b8-1f2b-4056-fbde-e46a91d99966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test mean = \n",
            "radius_mean                0.020907\n",
            "texture_mean               0.164319\n",
            "perimeter_mean             0.030981\n",
            "area_mean                  0.014927\n",
            "smoothness_mean           -0.019211\n",
            "compactness_mean           0.101172\n",
            "concavity_mean             0.084439\n",
            "concave points_mean        0.029660\n",
            "symmetry_mean              0.040621\n",
            "fractal_dimension_mean     0.094946\n",
            "radius_se                  0.046538\n",
            "texture_se                 0.064287\n",
            "perimeter_se               0.100075\n",
            "area_se                    0.005387\n",
            "smoothness_se              0.066020\n",
            "compactness_se             0.118395\n",
            "concavity_se               0.067615\n",
            "concave points_se          0.126894\n",
            "symmetry_se                0.010380\n",
            "fractal_dimension_se       0.161937\n",
            "radius_worst               0.013377\n",
            "texture_worst              0.113419\n",
            "perimeter_worst            0.033903\n",
            "area_worst                -0.005170\n",
            "smoothness_worst          -0.020016\n",
            "compactness_worst          0.078428\n",
            "concavity_worst            0.117564\n",
            "concave points_worst       0.012128\n",
            "symmetry_worst            -0.027007\n",
            "fractal_dimension_worst    0.127447\n",
            "dtype: float64\n",
            "test std = \n",
            "radius_mean                0.984840\n",
            "texture_mean               1.152213\n",
            "perimeter_mean             0.997834\n",
            "area_mean                  0.910614\n",
            "smoothness_mean            1.093246\n",
            "compactness_mean           1.209947\n",
            "concavity_mean             1.116129\n",
            "concave points_mean        1.026998\n",
            "symmetry_mean              1.003385\n",
            "fractal_dimension_mean     1.163879\n",
            "radius_se                  0.869296\n",
            "texture_se                 1.085816\n",
            "perimeter_se               0.957380\n",
            "area_se                    0.737976\n",
            "smoothness_se              1.253428\n",
            "compactness_se             1.104842\n",
            "concavity_se               0.841179\n",
            "concave points_se          1.209135\n",
            "symmetry_se                1.042763\n",
            "fractal_dimension_se       1.058563\n",
            "radius_worst               0.941122\n",
            "texture_worst              1.032902\n",
            "perimeter_worst            0.960713\n",
            "area_worst                 0.846294\n",
            "smoothness_worst           1.045669\n",
            "compactness_worst          1.129954\n",
            "concavity_worst            1.204499\n",
            "concave points_worst       1.032070\n",
            "symmetry_worst             0.908280\n",
            "fractal_dimension_worst    1.062407\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = np.mean(x_train, axis=0).values.reshape(1, d)\n",
        "sig = np.std(x_train, axis=0).values.reshape(1, d)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(np.mean(x_test, axis=0))\n",
        "\n",
        "print('test std = ')\n",
        "print(np.std(x_test, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, d = x_train.shape\n",
        "x_train = np.concatenate((x_train, np.ones((train, 1))), axis=1)\n",
        "test, d = x_test.shape\n",
        "x_test = np.concatenate((x_test, np.ones((test, 1))), axis=1\n",
        "                      )"
      ],
      "metadata": {
        "id": "5axhmPhud1KQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YV0Zy5KbIim"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "L-Ax01u8bIin"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "def objective(w, x, y, lam):\n",
        "  yxT = np.multiply(y, x)\n",
        "  yxTw = np.dot(yxT, w)\n",
        "  Q = np.mean(np.log(1 + np.exp(-yxTw))) + ((lam / 2) * np.square(np.sum(w)))\n",
        "  \n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1mhJ9IbIio"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk4PO1nHbIip"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2CZMYwGbIip"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pTvuBZXEbIiq"
      },
      "outputs": [],
      "source": [
        "# Calculate the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "  # print(w.shape, x.shape)\n",
        "  yxT = np.multiply(y, x)\n",
        "  yxTw = np.dot(yxT, w)\n",
        "\n",
        "  g = -np.mean(np.divide(yxT,(1 + np.exp(yxTw))),axis = 0).reshape(x.shape[1],1) + lam * w\n",
        "  \n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "tf9eGWHnbIir"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, max_epoch=100):\n",
        "    objvals = np.zeros(max_epoch)\n",
        "\n",
        "    w = np.zeros((x.shape[1], 1))\n",
        "    \n",
        "\n",
        "    y = np.array(y).reshape(x.shape[0],1)\n",
        "        \n",
        "    for i in range(max_epoch):\n",
        "        g = gradient(w, x, y, lam)\n",
        "        w -= learning_rate * g\n",
        "        objval = objective(w, x, y, lam)\n",
        "        objvals[i] = objval\n",
        "    return w, objvals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNso1fN5bIis"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ChHGMRsPbIit"
      },
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "w, objvals = gradient_descent(x_train, y_train, 0, 0.1,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "SvUW66kIbIit"
      },
      "outputs": [],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 1E-6\n",
        "w_reg, objvals_reg = gradient_descent(x_train, y_train, lam, 0.1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVNzctZUbIiu"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uI4FKv9hbIiv"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "  yixi = np.multiply(yi,xi)\n",
        "  yixiw = np.dot(yixi,w)\n",
        "  a = np.log(1+ np.exp(-yixiw))\n",
        "  b = (lam/2) * np.sum(np.square(w))\n",
        "  obj = a + b\n",
        "  c = -(np.multiply(yi,xi)) / (1+ np.exp(yixiw)) \n",
        "  d = (lam * w).reshape(xi.shape[0])\n",
        "  g = c + d\n",
        "  # print(g.shape)\n",
        "  return obj, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnqvFP1abIiv"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples.\n",
        "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "6pI065OdbIiw"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     \n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, max_epoch=100):\n",
        "  objvals = [0] * max_epoch\n",
        "  # print(y.shape)\n",
        "  w = np.zeros((x.shape[1],1)) \n",
        "  \n",
        "  y = np.array(y).reshape((x.shape[0], 1))\n",
        "\n",
        "  for i in range(max_epoch):\n",
        "    random_samples = np.random.permutation(x.shape[0])\n",
        "    x_rand_samples = x[random_samples,: ]\n",
        "    y_rand_samples = y[random_samples,:]\n",
        "\n",
        "    objval = 0\n",
        "    for j in range(x.shape[0]):\n",
        "      xi = x_rand_samples[j,:]\n",
        "      # print(xi.shape)\n",
        "      yi = float(y_rand_samples[j,:])\n",
        "      obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "      objval += obj\n",
        "      w-= (learning_rate * g.reshape((x.shape[1],1)))     \n",
        "    objval /= x.shape[0]\n",
        "    objvals[i] = objval\n",
        "\n",
        "  return w, objvals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcD5eGR4bIix"
      },
      "source": [
        "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "BaQo7oMEbIix"
      },
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "w_sgd, objvals_sgd = sgd(x_train, y_train, 0, 0.1, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "YxXOHGmVbIix"
      },
      "outputs": [],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "w_sgd_reg, objvals_sgd_reg = gradient_descent(x_train, y_train, lam, 0.1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlFsSEoXbIiy"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcNPs4-ebIiy"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ESj76P-BbIiz"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-b matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "  yixi = np.multiply(yi,xi)\n",
        "  yixiw = np.dot(yixi,w)\n",
        "  obj = np.mean(np.log(1 + np.exp(-yixiw))) + (lam / 2) * np.sum(np.square(w))\n",
        "\n",
        "  g = np.mean(np.divide(-yixi , (1 + np.exp(yixiw))),axis = 0).reshape(xi.shape[1],1) + lam * w\n",
        "\n",
        "  return obj, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMfoN7ibIiz"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
        "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "5GEzanzHbIi0"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate,batch_size,max_epoch=100):\n",
        "  objvals = [0] * max_epoch\n",
        "  b = 20\n",
        "  # print(y.shape)\n",
        "  w = np.zeros((x_train.shape[1], 1))\n",
        "  \n",
        "  y = np.array(y).reshape((x.shape[0], 1))\n",
        "\n",
        "  for i in range(max_epoch):\n",
        "    random_samples = np.random.permutation(x.shape[0])\n",
        "    x_rand_samples = x[random_samples,: ]\n",
        "    y_rand_samples = y[random_samples,:]\n",
        "\n",
        "    objval = 0\n",
        "    for k in range(x.shape[0] // b):\n",
        "      xBatch = x_rand_samples[k * b: k * b + b]\n",
        "      # print(x.shape)\n",
        "      yBatch = y_rand_samples[k * b: k * b + b]\n",
        "      obj, g = mb_objective_gradient(w, xBatch, yBatch, lam)\n",
        "      objval += obj\n",
        "      temp = np.multiply(learning_rate, g)\n",
        "      w -= temp\n",
        "    \n",
        "    objval /= b\n",
        "    objvals[i] = objval\n",
        "\n",
        "  return w, objvals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UesJI97YbIi0"
      },
      "source": [
        "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "XXvzNVNDbIi1"
      },
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "w_mbgd, objvals_mbgd = mbgd(x_train, y_train, 0, 0.1, 50, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "sPUdhdySbIi2"
      },
      "outputs": [],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "w_mbgd_reg, objvals_mbgd_reg = mbgd(x_train, y_train, 1E-6, 0.1, 50, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJYskEnrbIi2"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "2gvbMDFlbIi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b1322778-d312-4f60-ec2a-aca5f73987a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "epochs_gd = range(len(objvals))\n",
        "epochs_sgd = range(len(objvals_sgd))\n",
        "epochs_mbgd = range(len(objvals_mbgd))\n",
        "\n",
        "l1, = plt.plot(epochs_gd, objvals)\n",
        "l2, = plt.plot(epochs_sgd, objvals_sgd)\n",
        "l3, = plt.plot(epochs_mbgd, objvals_mbgd)\n",
        "plt.title(\"Objective Function vs Epochs\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Objective Function Value')\n",
        "plt.legend([l1, l2, l3], ['Gradient descent', 'Stochastic Gradient descent', 'Mini-Batch Gradient descent'])\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fCS0ECbIi3"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "jAxkJrILbIi4"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "def predict(w, X):\n",
        "    xw = (np.dot(X, w))\n",
        "    f = np.array([1 if i > 0 else -1 for i in xw])\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "2iL3KiCWbIi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5130d6d-ca99-4ea1-e15c-12fb45345e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for GD is: 0.989010989010989\n",
            "Accuracy for GD regularized is: 0.989010989010989\n",
            "Accuracy for SGD is: 0.989010989010989\n",
            "Accuracy for SGD regularized is: 0.989010989010989\n",
            "Accuracy for MBGD is: 0.989010989010989\n",
            "Accuracy for MBGD regularized is: 0.9868131868131869\n"
          ]
        }
      ],
      "source": [
        "# evaluate training error of logistric regression and regularized version\n",
        "# GD\n",
        "predict_train = predict(w, x_train)\n",
        "error = np.mean(np.abs(predict_train - y_train) / 2)\n",
        "print(\"Accuracy for GD is: \" + str(1 - error))\n",
        "# GD regularized\n",
        "predict_train_reg = predict(w_reg, x_train)\n",
        "error_reg = np.mean(np.abs(predict_train_reg - y_train) / 2)\n",
        "print(\"Accuracy for GD regularized is: \" + str(1 - error_reg))\n",
        "# SGD\n",
        "predict_train_sgd = predict(w_sgd, x_train)\n",
        "error_sgd = np.mean(np.abs(predict_train_sgd - y_train) / 2)\n",
        "print(\"Accuracy for SGD is: \" + str(1 - error_sgd))\n",
        "# SGD regularized\n",
        "predict_train_sgd_reg = predict(w_sgd_reg, x_train)\n",
        "error_sgd_reg = np.mean(np.abs(predict_train_sgd_reg - y_train) / 2)\n",
        "print(\"Accuracy for SGD regularized is: \" + str(1 - error_sgd_reg))\n",
        "# MBGD\n",
        "predict_train_mbgd = predict(w_mbgd, x_train)\n",
        "error_mbgd = np.mean(np.abs(predict_train_sgd - y_train) / 2)\n",
        "print(\"Accuracy for MBGD is: \" + str(1 - error_mbgd))\n",
        "# MBGD regularized\n",
        "predict_train_mbgd_reg = predict(w_mbgd_reg, x_train)\n",
        "error_mbgd_reg = np.mean(np.abs(predict_train_mbgd_reg - y_train) / 2)\n",
        "print(\"Accuracy for MBGD regularized is: \" + str(1 - error_mbgd_reg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "N8-ViV3JbIi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37806900-2837-4ef8-d715-4dfe2801046e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for GD is: 0.9649122807017544\n",
            "Accuracy for GD regularized is: 0.9649122807017544\n",
            "Accuracy for SGD is: 0.9473684210526316\n",
            "Accuracy for SGD regularized is: 0.9649122807017544\n",
            "Accuracy for MBGD is: 0.9473684210526316\n",
            "Accuracy for MBGD regularized is: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "# evaluate testing error of logistric regression and regularized version\n",
        "# GD\n",
        "predict_test = predict(w, x_test)\n",
        "error = np.mean(np.abs(predict_test - y_test) / 2)\n",
        "print(\"Accuracy for GD is: \" + str(1 - error))\n",
        "# GD regularized\n",
        "predict_test_reg = predict(w_reg, x_test)\n",
        "error = np.mean(np.abs(predict_test_reg - y_test) / 2)\n",
        "print(\"Accuracy for GD regularized is: \" + str(1 - error))\n",
        "# SGD\n",
        "predict_test_sgd = predict(w_sgd, x_test)\n",
        "error = np.mean(np.abs(predict_test_sgd - y_test) / 2)\n",
        "print(\"Accuracy for SGD is: \" + str(1 - error))\n",
        "# SGD regularized\n",
        "predict_test_sgd_reg = predict(w_sgd_reg, x_test)\n",
        "error = np.mean(np.abs(predict_test_sgd_reg - y_test) / 2)\n",
        "print(\"Accuracy for SGD regularized is: \" + str(1 - error))\n",
        "# MBGD\n",
        "predict_test_mbgd = predict(w_mbgd, x_test)\n",
        "error = np.mean(np.abs(predict_test_sgd - y_test) / 2)\n",
        "print(\"Accuracy for MBGD is: \" + str(1 - error))\n",
        "# MBGD regularized\n",
        "predict_test_mbgd_reg = predict(w_mbgd_reg, x_test)\n",
        "error = np.mean(np.abs(predict_test_mbgd_reg - y_test) / 2)\n",
        "print(\"Accuracy for MBGD regularized is: \" + str(1 - error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2c0LuLhbIi5"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT8by-UabIi5"
      },
      "source": [
        "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Kafq-EOybIi6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Vatsal_Shah_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}